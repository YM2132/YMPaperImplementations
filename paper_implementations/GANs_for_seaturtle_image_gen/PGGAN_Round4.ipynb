{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e4ed47f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# All the imports required for this implementation\n",
    "import torch\n",
    "import torchvision\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.nn.init as init\n",
    "from torch.nn.utils import spectral_norm\n",
    "import torch.autograd as autograd\n",
    "\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torchvision.utils import save_image\n",
    "\n",
    "from torch.utils.data import TensorDataset, ConcatDataset, random_split, DataLoader, Dataset\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "\n",
    "from torchinfo import summary # Allows us to summarise the params and layers\n",
    "\n",
    "from torch import nn, optim\n",
    "from torch.autograd import Variable, grad\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms, utils\n",
    "\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import copy\n",
    "import math\n",
    "import random\n",
    "import argparse\n",
    "import random\n",
    "import os\n",
    "import logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3bb3e6a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "# We can make use of a GPU if you have one on your computer. This works for Nvidia and M series GPU's\n",
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "    # These 2 lines assign some data on the memory of the device and output it. The output confirms\n",
    "    # if we have set the intended device\n",
    "    x = torch.ones(1, device=device)\n",
    "    print (x)\n",
    "elif torch.backends.cuda.is_built():\n",
    "    device = torch.device(\"cuda\")\n",
    "    x = torch.ones(1, device=device)\n",
    "    print (x)\n",
    "else:\n",
    "    device = (\"cpu\")\n",
    "    x = torch.ones(1, device=device)\n",
    "    print (x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cc0a0764",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to show an image\n",
    "def imshow(img):\n",
    "    img = img / 2 + 0.5  # unnormalize\n",
    "    npimg = img.numpy()\n",
    "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "    plt.show()\n",
    "\n",
    "def show_images(images, num_images=16, figsize=(10,10)):\n",
    "    # Ensure the input is on CPU\n",
    "    images = images.cpu().detach()\n",
    "    \n",
    "    # Normalize images from [-1, 1] to [0, 1]\n",
    "    images = (images + 1) / 2\n",
    "    \n",
    "    # Clamp values to [0, 1] range\n",
    "    images = torch.clamp(images, 0, 1)\n",
    "    \n",
    "    # Make a grid of images\n",
    "    grid = torchvision.utils.make_grid(images[:num_images], nrow=4)\n",
    "    \n",
    "    # Convert to numpy and transpose\n",
    "    grid = grid.numpy().transpose((1, 2, 0))\n",
    "    \n",
    "    # Display the grid\n",
    "    plt.figure(figsize=figsize)\n",
    "    plt.imshow(grid)\n",
    "    plt.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "80551ac4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test this reshape vs traditional reshape\n",
    "class Reshape(nn.Module):\n",
    "    def __init__(self, *args):\n",
    "        super().__init__()\n",
    "        self.shape = args\n",
    "    def forward(self, x):\n",
    "        return x.view(self.shape)\n",
    "    \n",
    "class EqualLRLinear(nn.Module):\n",
    "    def __init__(self, in_features, out_features, gain=np.sqrt(2)):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.gain = gain\n",
    "        \n",
    "        fan_in = in_features\n",
    "        self.he_std = gain / np.sqrt(fan_in) # This aint mentioned in the paper lololol\n",
    "        \n",
    "        init_std = 1.0 / self.he_std\n",
    "        self.weight = nn.Parameter(torch.randn(out_features, in_features) * init_std)\n",
    "        self.wscale = self.he_std\n",
    "        \n",
    "        self.bias = nn.Parameter(torch.zeros(out_features))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # if input not flattened\n",
    "        if len(x.shape) > 2:\n",
    "            x = x.view(x.size(0), -1)\n",
    "        \n",
    "        weight = self.weight * self.wscale\n",
    "        \n",
    "        return F.linear(x, weight, self.bias)\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return (f'{self.__class__.__name__}({self.in_features}, {self.out_features}, '\n",
    "                f'gain={self.gain})')\n",
    "\n",
    "class EqualLRConv2d(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, stride=1, padding='same', gain=np.sqrt(2)):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.gain = gain\n",
    "        self.kernel_size = kernel_size\n",
    "        self.stride = stride\n",
    "        self.padding = padding\n",
    "        \n",
    "        # ensure kernel_size is odd\n",
    "        #assert kernel_size >= 1 and kernel_size % 2 == 1\n",
    "        \n",
    "        # fan_in\n",
    "        fan_in = in_channels * kernel_size ** 2\n",
    "        he_std = gain / np.sqrt(fan_in)\n",
    "        \n",
    "        init_std = 1.0 / he_std\n",
    "        self.weight = nn.Parameter(torch.randn(out_channels, in_channels, kernel_size, kernel_size) * init_std)\n",
    "        self.wscale = he_std\n",
    "        \n",
    "        self.bias = nn.Parameter(torch.zeros(out_channels))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        weight = self.weight * self.wscale\n",
    "        \n",
    "        return F.conv2d(x, weight, self.bias, stride=self.stride, padding=self.padding)\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return (f'{self.__class__.__name__}({self.in_channels}, {self.out_channels}, '\n",
    "                f'kernel_size={self.kernel_size}, stride={self.stride}, padding={self.padding}, '\n",
    "                f'gain={self.gain})')\n",
    "\n",
    "class EqualLRConvTranspose2d(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, stride=1, padding=0, output_padding=0, gain=np.sqrt(2)):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.gain = gain\n",
    "        self.kernel_size = kernel_size\n",
    "        self.stride = stride\n",
    "        self.padding = padding\n",
    "        self.output_padding = output_padding\n",
    "        \n",
    "        # fan_in for transposed convolution is based on out_channels\n",
    "        fan_in = out_channels * kernel_size ** 2\n",
    "        he_std = gain / np.sqrt(fan_in)\n",
    "        \n",
    "        init_std = 1.0 / he_std\n",
    "        self.weight = nn.Parameter(torch.randn(in_channels, out_channels, kernel_size, kernel_size) * init_std)\n",
    "        self.wscale = he_std\n",
    "        \n",
    "        self.bias = nn.Parameter(torch.zeros(out_channels))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        weight = self.weight * self.wscale\n",
    "        \n",
    "        return F.conv_transpose2d(x, weight, self.bias, \n",
    "                                  stride=self.stride, \n",
    "                                  padding=self.padding, \n",
    "                                  output_padding=self.output_padding)\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return (f'{self.__class__.__name__}({self.in_channels}, {self.out_channels}, '\n",
    "                f'kernel_size={self.kernel_size}, stride={self.stride}, '\n",
    "                f'padding={self.padding}, output_padding={self.output_padding}, '\n",
    "                f'gain={self.gain})')\n",
    "    \n",
    "    \n",
    "class PixelNorm(nn.Module):\n",
    "    def __init__(self, epsilon=1e-8):\n",
    "        super().__init__()\n",
    "        self.epsilon = epsilon\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # paper uses x / but we use x * here?!?!\n",
    "        return x / torch.rsqrt(torch.mean(torch.square(x), dim=1, keepdim=True) + self.epsilon)\n",
    "    \n",
    "class MiniBatchStdDev(nn.Module):\n",
    "    def __init__(self, group_size=4):\n",
    "        super().__init__()\n",
    "        self.group_size = group_size\n",
    "    \n",
    "    def forward(self, x):\n",
    "        N, C, H, W = x.shape\n",
    "        G = min(self.group_size, N)\n",
    "        \n",
    "        y = x.view(G, -1, C, H, W)\n",
    "        \n",
    "        y = y - torch.mean(y, dim=0, keepdim=True)\n",
    "        y = torch.mean(torch.square(y), dim=0)\n",
    "        y = torch.sqrt(y + 1e-8)\n",
    "        y = torch.mean(y, dim=[1,2,3], keepdim=True)\n",
    "        \n",
    "        y = y.repeat(G, 1, H, W)\n",
    "        \n",
    "        return torch.cat([x,y], dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d629a834",
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import sqrt\n",
    "\n",
    "class EqualLR:\n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "\n",
    "    def compute_weight(self, module):\n",
    "        weight = getattr(module, self.name + '_orig')\n",
    "        fan_in = weight.data.size(1) * weight.data[0][0].numel()\n",
    "\n",
    "        return weight * sqrt(2 / (fan_in))\n",
    "\n",
    "    @staticmethod\n",
    "    def apply(module, name):\n",
    "        fn = EqualLR(name)\n",
    "\n",
    "        weight = getattr(module, name)\n",
    "        del module._parameters[name]\n",
    "        module.register_parameter(name + '_orig', nn.Parameter(weight.data))\n",
    "        module.register_forward_pre_hook(fn)\n",
    "\n",
    "        return fn\n",
    "\n",
    "    def __call__(self, module, input):\n",
    "        weight = self.compute_weight(module)\n",
    "        setattr(module, self.name, weight)\n",
    "\n",
    "\n",
    "def equal_lr(module, name='weight'):\n",
    "    EqualLR.apply(module, name)\n",
    "\n",
    "    return module\n",
    "\n",
    "\n",
    "class EqualLRConv2d(nn.Module):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__()\n",
    "\n",
    "        conv = nn.Conv2d(*args, **kwargs)\n",
    "        conv.weight.data.normal_()\n",
    "        conv.bias.data.zero_()\n",
    "        #nn.init.kaiming_normal_(conv.weight, mode='fan_out', nonlinearity='leaky_relu')\n",
    "        #nn.init.constant_(conv.bias, 0)\n",
    "        self.conv = equal_lr(conv)\n",
    "\n",
    "    def forward(self, input):\n",
    "        return self.conv(input)\n",
    "\n",
    "\n",
    "class EqualLRConvTranspose2d(nn.Module):\n",
    "    ### additional module for OOGAN usage\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__()\n",
    "\n",
    "        conv = nn.ConvTranspose2d(*args, **kwargs)\n",
    "        conv.weight.data.normal_()\n",
    "        conv.bias.data.zero_()\n",
    "        #nn.init.kaiming_normal_(conv.weight, mode='fan_out', nonlinearity='leaky_relu')\n",
    "        #nn.init.constant_(conv.bias, 0)\n",
    "        self.conv = equal_lr(conv)\n",
    "\n",
    "    def forward(self, input):\n",
    "        return self.conv(input)\n",
    "\n",
    "class EqualLRLinear(nn.Module):\n",
    "    def __init__(self, in_dim, out_dim):\n",
    "        super().__init__()\n",
    "\n",
    "        linear = nn.Linear(in_dim, out_dim)\n",
    "        linear.weight.data.normal_()\n",
    "        linear.bias.data.zero_()\n",
    "        #nn.init.kaiming_normal_(linear.weight, mode='fan_in', nonlinearity='leaky_relu')\n",
    "        #nn.init.constant_(linear.bias, 0)\n",
    "\n",
    "        self.linear = equal_lr(linear)\n",
    "\n",
    "    def forward(self, input):\n",
    "        return self.linear(input)\n",
    "    \n",
    "class PixelNorm(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # paper uses x / but we use x * here?!?!\n",
    "        return x / torch.sqrt(torch.mean(x ** 2, dim=1, keepdim=True)\n",
    "                                  + 1e-8)\n",
    "\n",
    "class MiniBatchStdDev(nn.Module):\n",
    "    def __init__(self, group_size=4):\n",
    "        super().__init__()\n",
    "        self.group_size = group_size\n",
    "    \n",
    "    def forward(self, x):\n",
    "        N, C, H, W = x.shape\n",
    "        G = min(self.group_size, N)\n",
    "        \n",
    "        y = x.view(G, -1, C, H, W)\n",
    "        \n",
    "        y = y - torch.mean(y, dim=0, keepdim=True)\n",
    "        y = torch.mean(torch.square(y), dim=0)\n",
    "        y = torch.sqrt(y + 1e-8)\n",
    "        y = torch.mean(y, dim=[1,2,3], keepdim=True)\n",
    "        \n",
    "        y = y.repeat(G, 1, H, W)\n",
    "        \n",
    "        return torch.cat([x,y], dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4d41058d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class G_ConvBlock(nn.Module):\n",
    "    def __init__(\n",
    "        self, \n",
    "        in_c, \n",
    "        out_c, \n",
    "        ksize1, \n",
    "        padding,\n",
    "        ksize2=None, \n",
    "        padding2=None,\n",
    "        stride=None, \n",
    "        use_fc=False\n",
    "    ):\n",
    "        super().__init__()\n",
    "        \n",
    "        layers_list = []\n",
    "        \n",
    "        if ksize2 is None:\n",
    "            ksize2 = ksize1\n",
    "            \n",
    "        if padding2 is None:\n",
    "            padding2 = padding\n",
    "        \n",
    "        if use_fc:\n",
    "            # Normalize input\n",
    "            # layers_list.append(PixelNorm())\n",
    "            \n",
    "            # Fully connected layer for input\n",
    "            #'''fc_out = out_c * 16\n",
    "            #layers_list.extend([\n",
    "            #    EqualLRLinear(in_features=512, out_features=fc_out, gain=np.sqrt(2)/4),\n",
    "            #    Reshape(-1, out_c, 4, 4),\n",
    "            #    nn.LeakyReLU(0.2),\n",
    "            #    PixelNorm(),\n",
    "            #])'''\n",
    "            layers_list.extend([\n",
    "                EqualLRConv2d(in_c, out_c, ksize1, padding=padding),\n",
    "                PixelNorm(),\n",
    "                nn.LeakyReLU(0.2),\n",
    "                #PixelNorm(),\n",
    "            ])\n",
    "            # Conv 3x3 layer\n",
    "            layers_list.extend([\n",
    "                EqualLRConv2d(out_c, out_c, ksize2, padding=padding2),\n",
    "                PixelNorm(),\n",
    "                nn.LeakyReLU(0.2),\n",
    "                #PixelNorm(),\n",
    "            ])\n",
    "        else:\n",
    "            layers_list.extend([\n",
    "                # Upscale\n",
    "                nn.Upsample(scale_factor=2, mode='nearest'),\n",
    "                EqualLRConv2d(in_c, out_c, ksize1, padding=padding),\n",
    "                PixelNorm(),\n",
    "                nn.LeakyReLU(0.2),\n",
    "                #PixelNorm(),\n",
    "                \n",
    "                EqualLRConv2d(out_c, out_c, ksize2, padding=padding2),\n",
    "                PixelNorm(),\n",
    "                nn.LeakyReLU(0.2),\n",
    "                #PixelNorm(),\n",
    "            ])\n",
    "        \n",
    "        self.layers = nn.ModuleList(layers_list)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        for layer in self.layers:\n",
    "            #print(layer)\n",
    "            x = layer(x)\n",
    "            #print(x.shape)\n",
    "        return x\n",
    "        #debug=True\n",
    "        #for i, layer in enumerate(self.layers):\n",
    "        #    x = layer(x)\n",
    "        #    if debug:\n",
    "        #        self.debug_output(x, f\"Layer {i}: {type(layer).__name__}\")\n",
    "        #return x\n",
    "    @staticmethod\n",
    "    def debug_output(tensor, layer_name):\n",
    "        if torch.isnan(tensor).any():\n",
    "            print(f\"NaN detected in {layer_name}\")\n",
    "            return\n",
    "\n",
    "        print(f\"\\nDebugging {layer_name} output:\")\n",
    "        print(f\"Shape: {tensor.shape}\")\n",
    "        print(f\"Dtype: {tensor.dtype}\")\n",
    "        print(f\"Device: {tensor.device}\")\n",
    "\n",
    "        try:\n",
    "            stats = {\n",
    "                \"Min\": tensor.min().item(),\n",
    "                \"Max\": tensor.max().item(),\n",
    "                \"Mean\": tensor.mean().item(),\n",
    "                \"Std\": tensor.std().item()\n",
    "            }\n",
    "            print(\"\\nParameter statistics:\")\n",
    "            for k, v in stats.items():\n",
    "                print(f\"{k}: {v:.6f}\")\n",
    "\n",
    "            print(\"Sample values:\")\n",
    "            print(tensor.view(-1)[:10])  # Print first 10 values\n",
    "\n",
    "            if tensor.grad is not None:\n",
    "                print(\"\\nGradient statistics:\")\n",
    "                grad_stats = {\n",
    "                    \"Grad Min\": tensor.grad.min().item(),\n",
    "                    \"Grad Max\": tensor.grad.max().item(),\n",
    "                    \"Grad Mean\": tensor.grad.mean().item(),\n",
    "                    \"Grad Std\": tensor.grad.std().item()\n",
    "                }\n",
    "                for k, v in grad_stats.items():\n",
    "                    print(f\"{k}: {v:.6f}\")\n",
    "            else:\n",
    "                print(\"\\nNo gradient computed yet\")\n",
    "\n",
    "        except RuntimeError as e:\n",
    "            print(f\"Error computing statistics: {str(e)}\")\n",
    "\n",
    "        print(\"=\" * 50)\n",
    "            \n",
    "class D_ConvBlock(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_c,\n",
    "        out_c,\n",
    "        ksize1, \n",
    "        padding, \n",
    "        ksize2=None, \n",
    "        padding2=None,\n",
    "        stride=None,   \n",
    "        mbatch=None,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        \n",
    "        layers_list = []\n",
    "        \n",
    "        if ksize2 is None:\n",
    "            ksize2 = ksize1\n",
    "        if padding2 is None:\n",
    "            padding2 = padding\n",
    "        \n",
    "        if mbatch:\n",
    "            layers_list.extend([\n",
    "                MiniBatchStdDev(),\n",
    "                EqualLRConv2d(in_c, out_c, ksize1, padding=padding),\n",
    "                nn.LeakyReLU(0.2),\n",
    "                EqualLRConv2d(out_c, out_c, ksize2, padding=padding2),\n",
    "                nn.LeakyReLU(0.2),\n",
    "                #nn.Flatten(),\n",
    "                #EqualLRLinear(out_c*4*4, out_c),\n",
    "                #nn.LeakyReLU(0.2),\n",
    "                #EqualLRLinear(out_c, 1, gain=1),\n",
    "            ])\n",
    "        else:\n",
    "            layers_list.extend([\n",
    "                EqualLRConv2d(in_c, out_c, ksize1, padding=padding),\n",
    "                nn.LeakyReLU(0.2),\n",
    "                EqualLRConv2d(out_c, out_c, ksize2, padding=padding2),\n",
    "                nn.LeakyReLU(0.2),\n",
    "                #nn.AvgPool2d(2),\n",
    "            ])\n",
    "        \n",
    "        self.layers = nn.ModuleList(layers_list)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        for layer in self.layers:\n",
    "            #print(layer)\n",
    "            x = layer(x)\n",
    "            #print(x.shape)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ca2361d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Credit to: https://github.com/odegeasslbc/Progressive-GAN-pytorch/blob/master/progan_modules.py#L161\n",
    "\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, in_c=128):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.in_c = in_c\n",
    "        \n",
    "        self.input_layer = nn.Sequential(\n",
    "            EqualLRConvTranspose2d(in_c, in_c, 4, 1, 0),\n",
    "            PixelNorm(),\n",
    "            nn.LeakyReLU(0.2))\n",
    "        \n",
    "        self.block_4x4 = G_ConvBlock(in_c, in_c, 3, 1, use_fc=True)\n",
    "        self.block_8x8 = G_ConvBlock(in_c, in_c, 3, 1)\n",
    "        self.block_16x16 = G_ConvBlock(in_c, in_c, 3, 1)\n",
    "        self.block_32x32 = G_ConvBlock(in_c, in_c, 3, 1)\n",
    "        self.block_64x64 = G_ConvBlock(in_c, in_c//2, 3, 1)\n",
    "        self.block_128x128 = G_ConvBlock(in_c//2, in_c//4, 3, 1)\n",
    "        self.block_256x256 = G_ConvBlock(in_c//4, in_c//4, 3, 1)\n",
    "        \n",
    "        # no LeakyReLU on the to_RGBs\n",
    "        self.to_rgb_4 = EqualLRConv2d(in_c, 3, 1)\n",
    "        self.to_rgb_8 = EqualLRConv2d(in_c, 3, 1)\n",
    "        self.to_rgb_16 = EqualLRConv2d(in_c, 3, 1)\n",
    "        self.to_rgb_32 = EqualLRConv2d(in_c, 3, 1)\n",
    "        self.to_rgb_64 = EqualLRConv2d(in_c//2, 3, 1)\n",
    "        self.to_rgb_128 = EqualLRConv2d(in_c//4, 3, 1)\n",
    "        self.to_rgb_256 = EqualLRConv2d(in_c//4, 3, 1)\n",
    "                \n",
    "        self.tanh = nn.Tanh()\n",
    "    \n",
    "    def debug_tensor(self, tensor, name):\n",
    "        print(f\"\\nDebugging {name}:\")\n",
    "        print(f\"Shape: {tensor.shape}\")\n",
    "        print(f\"Dtype: {tensor.dtype}\")\n",
    "        print(f\"Device: {tensor.device}\")\n",
    "\n",
    "        if torch.isnan(tensor).any():\n",
    "            print(\"NaN detected\")\n",
    "        elif torch.isinf(tensor).any():\n",
    "            print(\"Inf detected\")\n",
    "\n",
    "        try:\n",
    "            print(\"\\nParameter statistics:\")\n",
    "            print(f\"Min: {tensor.min().item():.6f}\")\n",
    "            print(f\"Max: {tensor.max().item():.6f}\")\n",
    "            print(f\"Mean: {tensor.mean().item():.6f}\")\n",
    "            print(f\"Std: {tensor.std().item():.6f}\")\n",
    "        except RuntimeError as e:\n",
    "            print(f\"Unable to compute statistics: {str(e)}\")\n",
    "\n",
    "        print(\"Sample values:\")\n",
    "        if tensor.numel() > 10:\n",
    "            print(tensor.view(-1)[:10])\n",
    "        else:\n",
    "            print(tensor)\n",
    "\n",
    "        if tensor.requires_grad:\n",
    "            if tensor.grad is not None:\n",
    "                print(\"\\nGradient statistics:\")\n",
    "                grad = tensor.grad\n",
    "                print(f\"Grad shape: {grad.shape}\")\n",
    "                if torch.isnan(grad).any():\n",
    "                    print(\"NaN detected in gradient\")\n",
    "                elif torch.isinf(grad).any():\n",
    "                    print(\"Inf detected in gradient\")\n",
    "                try:\n",
    "                    print(f\"Grad min: {grad.min().item():.6f}\")\n",
    "                    print(f\"Grad max: {grad.max().item():.6f}\")\n",
    "                    print(f\"Grad mean: {grad.mean().item():.6f}\")\n",
    "                    print(f\"Grad std: {grad.std().item():.6f}\")\n",
    "                    print(\"Grad sample:\")\n",
    "                    print(grad.view(-1)[:10])\n",
    "                except RuntimeError as e:\n",
    "                    print(f\"Unable to compute gradient statistics: {str(e)}\")\n",
    "            else:\n",
    "                print(\"No gradient computed yet\")\n",
    "\n",
    "        print(\"\\n\" + \"=\"*50)\n",
    "\n",
    "    def forward(self, x, layer_num, alpha):        \n",
    "        #if torch.isnan(x).any():\n",
    "        #    print(\"NaN in input X\")\n",
    "        #self.debug_tensor(x, \"input\")\n",
    "        out_4 = self.input_layer(x.view(-1, self.in_c, 1, 1))\n",
    "        #self.debug_tensor(out_4, \"input_layer output\")\n",
    "        \n",
    "        out_4 = self.block_4x4(out_4)\n",
    "        #self.debug_tensor(out_4, \"block_4x4 output\")\n",
    "        if layer_num == 1:\n",
    "            out = self.to_rgb_4(out_4)\n",
    "        #    self.debug_tensor(out, \"to_rgb_4 output\")\n",
    "            out = self.tanh(out)\n",
    "        #    self.debug_tensor(out, \"final output (layer 1)\")\n",
    "            return out\n",
    "        \n",
    "        out_8 = self.block_8x8(out_4)\n",
    "       # self.debug_tensor(out_8, \"block_8x8 output\")\n",
    "        if layer_num == 2:\n",
    "            # Pass out_4 through rgb4 and upscale it\n",
    "            skip = self.to_rgb_4(out_4)\n",
    "       #     self.debug_tensor(skip, \"to_rgb_4 output\")\n",
    "            skip = F.interpolate(skip, scale_factor=2, mode='bilinear')\n",
    "       #     self.debug_tensor(skip, \"interpolated skip\")\n",
    "            out = self.to_rgb_8(out_8)\n",
    "       #     self.debug_tensor(out, \"to_rgb_8 output\")\n",
    "            \n",
    "            out = ((1-alpha) * skip) + (alpha * out)\n",
    "       #     self.debug_tensor(out, \"blended output\")\n",
    "            out = self.tanh(out)\n",
    "       #     self.debug_tensor(out, \"final output (layer 2)\")\n",
    "            return self.tanh(out)\n",
    "        \n",
    "        out_16 = self.block_16x16(out_8)\n",
    "        #self.debug_tensor(out_16, \"block_16x16 output\")\n",
    "        if layer_num == 3:\n",
    "            skip = self.to_rgb_8(out_8)\n",
    "        #    self.debug_tensor(skip, \"to_rgb_8 output\")\n",
    "            skip = F.interpolate(skip, scale_factor=2, mode='bilinear')\n",
    "        #    self.debug_tensor(skip, \"interpolated skip\")\n",
    "            out = self.to_rgb_16(out_16)\n",
    "        #    self.debug_tensor(out, \"to_rgb_16 output\")\n",
    "            \n",
    "            out = ((1-alpha) * skip) + (alpha * out)\n",
    "        #    self.debug_tensor(out, \"blended output\")\n",
    "            out = self.tanh(out)\n",
    "        #    self.debug_tensor(out, \"final output (layer 3)\")\n",
    "            return out\n",
    "        \n",
    "        out_32 = self.block_32x32(out_16)\n",
    "        #self.debug_tensor(out_16, \"block_32x32 output\")\n",
    "        if layer_num == 4:\n",
    "            skip = self.to_rgb_16(out_16)\n",
    "        #    self.debug_tensor(skip, \"to_rgb_16 output\")\n",
    "            skip = F.interpolate(skip, scale_factor=2, mode='bilinear')\n",
    "        #    self.debug_tensor(skip, \"interpolated skip\")\n",
    "            out = self.to_rgb_32(out_32)\n",
    "        #    self.debug_tensor(out, \"to_rgb_32 output\")\n",
    "            \n",
    "            out = ((1-alpha) * skip) + (alpha * out)\n",
    "        #    self.debug_tensor(out, \"blended output\")\n",
    "            out = self.tanh(out)\n",
    "        #    self.debug_tensor(out, \"final output (layer 4)\")\n",
    "            return out\n",
    "        \n",
    "        out_64 = self.block_64x64(out_32)\n",
    "        if layer_num == 5:\n",
    "            skip = self.to_rgb_32(out_32)\n",
    "            skip = F.interpolate(skip, scale_factor=2, mode='bilinear')\n",
    "            out = self.to_rgb_64(out_64)\n",
    "            \n",
    "            out = ((1-alpha) * skip) + (alpha * out)\n",
    "            return self.tanh(out)\n",
    "        \n",
    "        out_128 = self.block_128x128(out_64)\n",
    "        if layer_num == 6:\n",
    "            skip = self.to_rgb_64(out_64)\n",
    "            skip = F.interpolate(skip, scale_factor=2, mode='bilinear')\n",
    "            out = self.to_rgb_128(out_128)\n",
    "            \n",
    "            out = ((1-alpha) * skip) + (alpha * out)\n",
    "            return self.tanh(out)\n",
    "        \n",
    "        out_256 = self.block_256x256(out_128)\n",
    "        if layer_num == 7:\n",
    "            skip = self.to_rgb_128(out_128)\n",
    "            skip = F.interpolate(skip, scale_factor=2, mode='bilinear')\n",
    "            out = self.to_rgb_256(out_256)\n",
    "            \n",
    "            out = ((1-alpha) * skip) + (alpha * out)\n",
    "            return self.tanh(out)\n",
    "\n",
    "g = Generator().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "15f067ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x1000 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(None, torch.Size([1, 3, 4, 4]))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TESTING G\n",
    "g_in = torch.randn((1, 128, 1, 1), device=device)\n",
    "g_out = g(g_in, alpha=0.5, layer_num=1)\n",
    "show_images(g_out), g_out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5ac56b00",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, out_c=128):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.blocks = nn.ModuleList([\n",
    "            D_ConvBlock(out_c//4, out_c//4, 3, 1),\n",
    "            D_ConvBlock(out_c//4, out_c//2, 3, 1),\n",
    "            D_ConvBlock(out_c//2, out_c, 3, 1),\n",
    "            D_ConvBlock(out_c, out_c, 3, 1),\n",
    "            D_ConvBlock(out_c, out_c, 3, 1),\n",
    "            D_ConvBlock(out_c, out_c, 3, 1),\n",
    "            D_ConvBlock(out_c+1, out_c, 3, 1, 4, 0, mbatch=True),\n",
    "        ])\n",
    "        \n",
    "        self.from_rgb = nn.ModuleList([\n",
    "            EqualLRConv2d(3, out_c//4, 1),\n",
    "            EqualLRConv2d(3, out_c//4, 1),\n",
    "            EqualLRConv2d(3, out_c//2, 1),\n",
    "            EqualLRConv2d(3, out_c, 1),\n",
    "            EqualLRConv2d(3, out_c, 1),\n",
    "            EqualLRConv2d(3, out_c, 1),\n",
    "            EqualLRConv2d(3, out_c, 1),\n",
    "        ])\n",
    "        \n",
    "        self.num_layers = len(self.blocks)\n",
    "        \n",
    "        self.linear = EqualLRLinear(out_c, 1)\n",
    "    \n",
    "    def forward(self, x, layer_num, alpha):\n",
    "        for i in reversed(range(layer_num)):\n",
    "            idx = self.num_layers - i - 1\n",
    "            #print(x.shape)\n",
    "            if i+1 == layer_num:\n",
    "                out = self.from_rgb[idx](x)\n",
    "                #print()\n",
    "            out = self.blocks[idx](out)\n",
    "            \n",
    "            if i > 0:\n",
    "                out = F.interpolate(out, scale_factor=0.5, mode='bilinear')\n",
    "                \n",
    "                if i+1 == layer_num and 0 <= alpha < 1:\n",
    "                    #skip = F.interpolate(x, scale_factor=0.5, mode='bilinear', align_corners=False)\n",
    "                    skip = F.interpolate(x, scale_factor=0.5, mode='bilinear')\n",
    "                    skip = self.from_rgb[idx + 1](skip)\n",
    "                    out = (1 - alpha) * skip + alpha * out\n",
    "        \n",
    "        #out = out.squeeze(2).squeeze(2)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.linear(out)\n",
    "        \n",
    "        return out\n",
    "\n",
    "d = Discriminator().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "025536cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0.1139]], device='cuda:0', grad_fn=<AddmmBackward0>),\n",
       " torch.Size([1, 1]))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d_in = torch.randn((1, 3, 32, 32), device=device)\n",
    "d_out = d(d_in, alpha=0.5, layer_num=4)\n",
    "d_out, d_out.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "967ec19f",
   "metadata": {},
   "source": [
    "Training loop\n",
    "from: https://github.com/odegeasslbc/Progressive-GAN-pytorch/blob/master/train.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e01ea3b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def accumulate(model1, model2, decay=0.999):\n",
    "    par1 = dict(model1.named_parameters())\n",
    "    par2 = dict(model2.named_parameters())\n",
    "\n",
    "    for k in par1.keys():\n",
    "        par1[k].data.mul_(decay).add_(1 - decay, par2[k].data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "65c4ec06",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 16\n",
    "def get_dataloader(image_size, batch_size=32):\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((image_size, image_size)),  # Resize images to the required size\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "    ])\n",
    "\n",
    "    dataset = ImageFolder(root='./celeba_hq_256', transform=transform)\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True, num_workers=4, drop_last=True)\n",
    "\n",
    "    return dataloader\n",
    "test_dataloader = get_dataloader\n",
    "\n",
    "def imagefolder_loader(path):\n",
    "    def loader(transform):\n",
    "        data = datasets.ImageFolder(path, transform=transform)\n",
    "        data_loader = DataLoader(data, shuffle=True, batch_size=batch_size,\n",
    "                                 num_workers=4)\n",
    "        return data_loader\n",
    "    return loader\n",
    "\n",
    "def sample_data(dataloader, img_size=4):\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize(img_size+int(img_size*0.2)+1),  # Resize images to the required size\n",
    "        transforms.RandomCrop(img_size),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "    ])\n",
    "    \n",
    "    loader = dataloader(transform)\n",
    "    \n",
    "    return loader\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "86ddd216",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(g, d, init_layer_num, loader, total_iter=600000, checkpoint_dir='./checkpoints', sample_dir='./samples', resume_checkpoint=None):\n",
    "    # Dynamically create new checkpoint dir with timestamp\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    checkpoint_dir = os.path.join(checkpoint_dir, f\"checkpoint_{timestamp}\")\n",
    "    sample_dir = os.path.join(sample_dir, f\"sample_{timestamp}\")\n",
    "    os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "    os.makedirs(sample_dir, exist_ok=True)\n",
    "    \n",
    "    img_size = [4, 8, 16, 32, 64, 128, 256]\n",
    "    \n",
    "    layer_num = init_layer_num\n",
    "    start_iter = 0\n",
    "    \n",
    "    # Resume from checkpoint if provided\n",
    "    if resume_checkpoint and os.path.isfile(resume_checkpoint):\n",
    "        print(f\"Loading checkpoint from {resume_checkpoint}\")\n",
    "        checkpoint = torch.load(resume_checkpoint)\n",
    "        g.load_state_dict(checkpoint['g_state_dict'])\n",
    "        d.load_state_dict(checkpoint['d_state_dict'])\n",
    "        g_optimizer.load_state_dict(checkpoint['g_optimizer_state_dict'])\n",
    "        d_optimizer.load_state_dict(checkpoint['d_optimizer_state_dict'])\n",
    "        layer_num = checkpoint['layer_num']\n",
    "        start_iter = checkpoint['iteration'] + 1\n",
    "        print(f\"Resuming from layer {layer_num}, iteration {start_iter}\")\n",
    "    \n",
    "    data_loader = sample_data(loader, img_size[layer_num-1])\n",
    "    dataset = iter(data_loader)\n",
    "    \n",
    "    # may need to set total_iter//6 to //7?\n",
    "    total_iter_remain = total_iter - (total_iter//7)*(layer_num-1)\n",
    "    \n",
    "    pbar = tqdm(range(total_iter_remain))\n",
    "    \n",
    "    disc_loss_val = 0\n",
    "    gen_loss_val = 0\n",
    "    grad_loss_val = 0\n",
    "    \n",
    "    alpha = 0\n",
    "    one = torch.tensor(1, dtype=torch.float).to(device)\n",
    "    mone = one * -1\n",
    "    iteration = 0\n",
    "    \n",
    "    # Set up logging in the new checkpoint directory\n",
    "    log_file = os.path.join(checkpoint_dir, 'training.log')\n",
    "    logging.basicConfig(filename=log_file, level=logging.INFO, \n",
    "                        format='%(asctime)s - %(message)s',\n",
    "                        datefmt='%Y-%m-%d %H:%M:%S')\n",
    "    \n",
    "    logging.info(f\"Starting training session. Checkpoint directory: {checkpoint_dir}\")\n",
    "\n",
    "    # Initialize lists to store loss values for statistics\n",
    "    disc_loss_history = []\n",
    "    gen_loss_history = []\n",
    "    grad_loss_history = []\n",
    "    \n",
    "    for i in pbar:\n",
    "        try:\n",
    "            d.zero_grad()\n",
    "\n",
    "            alpha = min(1, (2/(total_iter//7)) * iteration)\n",
    "\n",
    "            if iteration > total_iter//7:\n",
    "                #print(f\"Before transition - Layer: {layer_num}, Alpha: {alpha}, Iteration: {iteration}\")\n",
    "                alpha = 0\n",
    "                iteration = 0\n",
    "                layer_num += 1\n",
    "\n",
    "                if layer_num > 7:\n",
    "                    alpha = 1\n",
    "                    layer_num = 7\n",
    "                data_loader = sample_data(loader, img_size[layer_num-1])\n",
    "                dataset = iter(data_loader)\n",
    "                #print(f\"After transition - New Layer: {layer_num}, Alpha: {alpha}, Iteration: {iteration}\")\n",
    "                # Save checkpoint at transition\n",
    "                torch.save({\n",
    "                    'g_state_dict': g.state_dict(),\n",
    "                    'd_state_dict': d.state_dict(),\n",
    "                    'g_optimizer_state_dict': g_optimizer.state_dict(),\n",
    "                    'd_optimizer_state_dict': d_optimizer.state_dict(),\n",
    "                    'layer_num': layer_num,\n",
    "                    'iteration': i\n",
    "                }, f'{checkpoint_dir}/transition_layer_{layer_num}_iter_{i}.pth')\n",
    "\n",
    "            try:\n",
    "                real_img, label = next(dataset)\n",
    "            except (OSError, StopIteration):\n",
    "                dataset = iter(data_loader)\n",
    "                real_img, label = next(dataset)\n",
    "\n",
    "            iteration += 1\n",
    "\n",
    "            # Train D\n",
    "            b_size = real_img.size(0)\n",
    "            real_img = real_img.to(device)\n",
    "            label = label.to(device)\n",
    "            real_pred = d(real_img, layer_num=layer_num, alpha=alpha)\n",
    "            real_pred = real_pred.mean() - 0.001 * (real_pred**2).mean()\n",
    "            real_pred.backward(mone)\n",
    "\n",
    "            # sample input data for g\n",
    "            gen_z = torch.randn(b_size, 128).to(device)\n",
    "\n",
    "            fake_img = g(gen_z, layer_num=layer_num, alpha=alpha)\n",
    "            \n",
    "            #print(f\"Fake image stats - Min: {fake_img.min()}, Max: {fake_img.max()}, Mean: {fake_img.mean()}\")\n",
    "            #print(f\"Real image stats - Min: {real_img.min()}, Max: {real_img.max()}, Mean: {real_img.mean()}\")\n",
    "            \n",
    "            fake_pred = d(fake_img.detach(), layer_num=layer_num, alpha=alpha)\n",
    "            fake_pred = fake_pred.mean()\n",
    "            fake_pred.backward(one)\n",
    "            \n",
    "            #d_grad_norm = torch.nn.utils.clip_grad_norm_(d.parameters(), float('inf'))\n",
    "            \n",
    "            #print(f\"D(real) stats - Min: {real_pred.min()}, Max: {real_pred.max()}, Mean: {real_pred.mean()}\")\n",
    "            #print(f\"D(fake) stats - Min: {fake_pred.min()}, Max: {fake_pred.max()}, Mean: {fake_pred.mean()}\")\n",
    "            \n",
    "            # Grad penalty for D\n",
    "            eps = torch.rand(b_size, 1, 1, 1).to(device)\n",
    "            x_hat = eps * real_img.data + (1 - eps) * fake_img.detach().data\n",
    "            x_hat.requires_grad = True\n",
    "            hat_predict = d(x_hat, layer_num=layer_num, alpha=alpha)\n",
    "            grad_x_hat = grad(\n",
    "                outputs=hat_predict.sum(), inputs=x_hat, create_graph=True)[0]\n",
    "            grad_penalty = ((grad_x_hat.view(grad_x_hat.size(0), -1)\n",
    "                             .norm(2, dim=1) - 1)**2).mean()\n",
    "            grad_penalty = 10 * grad_penalty\n",
    "            grad_penalty.backward()\n",
    "            grad_loss_val += grad_penalty.item()\n",
    "            disc_loss_val += (real_pred - fake_pred).item()\n",
    "            \n",
    "            # After computing losses and before optimizer steps\n",
    "            #print(f\"Iteration {i+1}, Layer {layer_num} - D Loss: {disc_loss_val}, Grad Penalty: {grad_loss_val}\")\n",
    "            d_optimizer.step()\n",
    "\n",
    "            if (i+1) % n_critic == 0:\n",
    "                g.zero_grad()\n",
    "                d.zero_grad()\n",
    "\n",
    "                pred = d(fake_img, layer_num=layer_num, alpha=alpha)\n",
    "\n",
    "                loss = -pred.mean()\n",
    "                gen_loss_val += loss.item()\n",
    "\n",
    "                loss.backward()\n",
    "                #print(f'LOSS: {loss}')\n",
    "                #torch.nn.utils.clip_grad_norm_(g.parameters(), max_norm=1.0)\n",
    "                #g_grad_norm = torch.nn.utils.clip_grad_norm_(g.parameters(), float('inf'))\n",
    "                #print(f\"Gradient norms - D: {d_grad_norm}, G: {g_grad_norm}\")\n",
    "                # After computing losses and before optimizer steps\n",
    "                #print(f\"Iteration {i+1}, Layer {layer_num} - G Loss: {gen_loss_val}\")\n",
    "                #g_grad_norm = torch.nn.utils.clip_grad_norm_(g.parameters(), max_norm=0.5)\n",
    "                #print(f\"Gradient norm  after clip, G: {g_grad_norm}\")\n",
    "                #for name, param in g.named_parameters():\n",
    "                #    if torch.isnan(param).any():\n",
    "                #        print(f\"BEFORE OPTIM NaN detected in generator parameter: {name}\")\n",
    "                g_optimizer.step()\n",
    "                accumulate(g_running, g)\n",
    "                #for name, param in g.named_parameters():\n",
    "                #    if torch.isnan(param).any():\n",
    "                #        print(f\"NaN detected in generator parameter: {name}\")\n",
    "\n",
    "            # Check for NaN values after each iteration\n",
    "            if np.isnan(disc_loss_val) or np.isnan(gen_loss_val) or np.isnan(grad_loss_val):\n",
    "                logging.error(f\"NaN detected at iteration {i+1}\")\n",
    "                logging.error(f\"Disc Loss: {disc_loss_val}, Gen Loss: {gen_loss_val}, Grad Loss: {grad_loss_val}\")\n",
    "                logging.error(f\"Current layer: {layer_num}, Alpha: {alpha}\")\n",
    "\n",
    "                # Save final checkpoint before halting\n",
    "                torch.save({\n",
    "                    'g_state_dict': g.state_dict(),\n",
    "                    'd_state_dict': d.state_dict(),\n",
    "                    'g_optimizer_state_dict': g_optimizer.state_dict(),\n",
    "                    'd_optimizer_state_dict': d_optimizer.state_dict(),\n",
    "                    'layer_num': layer_num,\n",
    "                    'iteration': i\n",
    "                }, f'{checkpoint_dir}/nan_error_checkpoint_layer_{layer_num}_iter_{i+1}.pth')\n",
    "\n",
    "                print(\"NaN values detected. Training halted. Check the log file for details.\")\n",
    "                return\n",
    "\n",
    "            # Append loss values to history\n",
    "            disc_loss_history.append(disc_loss_val)\n",
    "            gen_loss_history.append(gen_loss_val)\n",
    "            grad_loss_history.append(grad_loss_val)\n",
    "\n",
    "            if (i+1)%500 == 0:\n",
    "                state_msg = (f'{i + 1}; G: {gen_loss_val/(500//n_critic):.3f}; D: {disc_loss_val/500:.3f};'\n",
    "                    f' Grad: {grad_loss_val/500:.3f}; Alpha: {alpha:.3f}')\n",
    "\n",
    "                 # Calculate and log statistics\n",
    "                avg_disc_loss = np.mean(disc_loss_history[-500:])\n",
    "                avg_gen_loss = np.mean(gen_loss_history[-500//n_critic:])\n",
    "                avg_grad_loss = np.mean(grad_loss_history[-500:])\n",
    "                \n",
    "                logging.info(state_msg)\n",
    "                logging.info(f\"Avg Disc Loss: {avg_disc_loss:.3f}, Avg Gen Loss: {avg_gen_loss:.3f}, Avg Grad Loss: {avg_grad_loss:.3f}\")\n",
    "                \n",
    "                disc_loss_val = 0\n",
    "                gen_loss_val = 0\n",
    "                grad_loss_val = 0\n",
    "\n",
    "                #print(state_msg)\n",
    "                pbar.set_description(state_msg)\n",
    "\n",
    "                with torch.no_grad():\n",
    "                    sample_z = torch.randn(4, 128).to(device)\n",
    "                    sample_imgs = g(sample_z, layer_num=layer_num, alpha=alpha)\n",
    "                    save_image(sample_imgs, f'{sample_dir}/sample_layer_{layer_num}_iter_{i+1}.png', nrow=4, normalize=True)\n",
    "                    show_images(sample_imgs)\n",
    "\n",
    "            # Save checkpoint every 10000 iterations\n",
    "            if (i+1) % 10000 == 0:\n",
    "                torch.save({\n",
    "                    'g_state_dict': g.state_dict(),\n",
    "                    'd_state_dict': d.state_dict(),\n",
    "                    'g_optimizer_state_dict': g_optimizer.state_dict(),\n",
    "                    'd_optimizer_state_dict': d_optimizer.state_dict(),\n",
    "                    'layer_num': layer_num,\n",
    "                    'iteration': i\n",
    "                }, f'{checkpoint_dir}/checkpoint_layer_{layer_num}_iter_{i+1}.pth')\n",
    "        \n",
    "            #print(f'Completed Iteration: {iteration}, Overall Iteration: {i+1}, Layer: {layer_num}, Alpha: {alpha:.3f}\\n')\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error occurred at iteration {i+1}: {str(e)}\")\n",
    "            raise\n",
    "\n",
    "    # Log final statistics\n",
    "    logging.info(\"Training completed.\")\n",
    "    logging.info(f\"Final layer: {layer_num}, Total iterations: {i+1}\")\n",
    "    logging.info(f\"Avg Disc Loss (last 1000): {np.mean(disc_loss_history[-1000:]):.3f}\")\n",
    "    logging.info(f\"Avg Gen Loss (last 1000): {np.mean(gen_loss_history[-1000:]):.3f}\")\n",
    "    logging.info(f\"Avg Grad Loss (last 1000): {np.mean(grad_loss_history[-1000:]):.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48b0063f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Args:\n",
    "    def __init__(self):\n",
    "        self.path = '/home/yusuf/python/YMPaperImplementations/paper_implementations/GANs_for_seaturtle_image_gen/celeba_hq_256'\n",
    "        self.trial_name = \"test1\"\n",
    "        self.gpu_id = 0\n",
    "        self.lr = 0.001\n",
    "        self.z_dim = 128\n",
    "        self.channel = 128\n",
    "        self.batch_size = 4\n",
    "        self.n_critic = 1\n",
    "        self.init_step = 1\n",
    "        self.total_iter = 700000\n",
    "        self.pixel_norm = False\n",
    "        self.tanh = False\n",
    "\n",
    "args = Args()\n",
    "\n",
    "trial_name = args.trial_name\n",
    "input_code_size = args.z_dim\n",
    "batch_size = args.batch_size\n",
    "n_critic = args.n_critic\n",
    "\n",
    "generator = Generator().to(device)\n",
    "discriminator = Discriminator().to(device)\n",
    "g_running = Generator().to(device)\n",
    "\n",
    "g_running.train(False)\n",
    "\n",
    "g_optimizer = torch.optim.Adam(generator.parameters(), lr=args.lr, betas=(0.0, 0.99))\n",
    "d_optimizer = torch.optim.Adam(discriminator.parameters(), lr=args.lr, betas=(0.0, 0.99))\n",
    "\n",
    "accumulate(g_running, generator, 0)\n",
    "\n",
    "loader = imagefolder_loader(args.path)\n",
    "\n",
    "#train(generator, discriminator, args.init_step, loader, args.total_iter, resume_checkpoint='./checkpoints/checkpoint_20240713_000009/transition_layer_4_iter_128574.pth')\n",
    "train(generator, discriminator, args.init_step, loader, args.total_iter)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad266438",
   "metadata": {},
   "source": [
    "## What now?\n",
    "    - Run with 256 latents\n",
    "    - Check why my EqualLR was bad\n",
    "    - Check if it was the same issue in previous code i had (i.e. was the model okay the LR the problem)\n",
    "    - Implement logging, saving checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ee23979",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets try with seaturtles!\n",
    "class Args:\n",
    "    def __init__(self):\n",
    "        self.path = '/home/yusuf/python/YMPaperImplementations/paper_implementations/GANs_for_seaturtle_image_gen/images'\n",
    "        self.trial_name = \"test1\"\n",
    "        self.gpu_id = 0\n",
    "        self.lr = 0.001\n",
    "        self.z_dim = 128\n",
    "        self.channel = 128\n",
    "        self.batch_size = 4\n",
    "        self.n_critic = 1\n",
    "        self.init_step = 1\n",
    "        self.total_iter = 700000\n",
    "        self.pixel_norm = False\n",
    "        self.tanh = False\n",
    "\n",
    "args = Args()\n",
    "\n",
    "trial_name = args.trial_name\n",
    "input_code_size = args.z_dim\n",
    "batch_size = args.batch_size\n",
    "n_critic = args.n_critic\n",
    "\n",
    "generator = Generator().to(device)\n",
    "discriminator = Discriminator().to(device)\n",
    "g_running = Generator().to(device)\n",
    "\n",
    "g_running.train(False)\n",
    "\n",
    "g_optimizer = torch.optim.Adam(generator.parameters(), lr=args.lr, betas=(0.0, 0.99))\n",
    "d_optimizer = torch.optim.Adam(discriminator.parameters(), lr=args.lr, betas=(0.0, 0.99))\n",
    "\n",
    "accumulate(g_running, generator, 0)\n",
    "\n",
    "loader = imagefolder_loader(args.path)\n",
    "\n",
    "#train(generator, discriminator, args.init_step, loader, args.total_iter, resume_checkpoint='./checkpoints/checkpoint_20240714_232605/checkpoint_layer_4_iter_260000.pth')\n",
    "train(generator, discriminator, args.init_step, loader, args.total_iter)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5a366c0",
   "metadata": {},
   "source": [
    "## Next steps for seaturtle gen\n",
    "    - Gen images with the left side flipped (makes them all right\n",
    "    side images)\n",
    "    - Remove the bad images from dataset\n",
    "    - Increase dim of images"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpu_use",
   "language": "python",
   "name": "gpu_use"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
