{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "97b5b553",
   "metadata": {},
   "outputs": [],
   "source": [
    "# All the imports required for this implementation\n",
    "import torch\n",
    "import torchvision\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.nn.init as init\n",
    "from torch.nn.utils import spectral_norm\n",
    "import torch.autograd as autograd\n",
    "\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.datasets import ImageFolder\n",
    "\n",
    "from torch.utils.data import TensorDataset, ConcatDataset, random_split, DataLoader, Dataset\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "\n",
    "from torchinfo import summary # Allows us to summarise the params and layers\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import copy\n",
    "import math\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "36ae716a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "# We can make use of a GPU if you have one on your computer. This works for Nvidia and M series GPU's\n",
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "    # These 2 lines assign some data on the memory of the device and output it. The output confirms\n",
    "    # if we have set the intended device\n",
    "    x = torch.ones(1, device=device)\n",
    "    print (x)\n",
    "elif torch.backends.cuda.is_built():\n",
    "    device = torch.device(\"cuda\")\n",
    "    x = torch.ones(1, device=device)\n",
    "    print (x)\n",
    "else:\n",
    "    device = (\"cpu\")\n",
    "    x = torch.ones(1, device=device)\n",
    "    print (x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "eb047378",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to show an image\n",
    "def imshow(img):\n",
    "    img = img / 2 + 0.5  # unnormalize\n",
    "    npimg = img.numpy()\n",
    "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "    plt.show()\n",
    "\n",
    "def show_images(images, num_images=16, figsize=(10,10)):\n",
    "    # Ensure the input is on CPU\n",
    "    images = images.cpu().detach()\n",
    "    \n",
    "    # Normalize images from [-1, 1] to [0, 1]\n",
    "    images = (images + 1) / 2\n",
    "    \n",
    "    # Clamp values to [0, 1] range\n",
    "    images = torch.clamp(images, 0, 1)\n",
    "    \n",
    "    # Make a grid of images\n",
    "    grid = torchvision.utils.make_grid(images[:num_images], nrow=4)\n",
    "    \n",
    "    # Convert to numpy and transpose\n",
    "    grid = grid.numpy().transpose((1, 2, 0))\n",
    "    \n",
    "    # Display the grid\n",
    "    plt.figure(figsize=figsize)\n",
    "    plt.imshow(grid)\n",
    "    plt.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "086b1296",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 16\n",
    "def get_dataloader(image_size, batch_size=32):\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((image_size, image_size)),  # Resize images to the required size\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "    ])\n",
    "\n",
    "    dataset = ImageFolder(root='./celeba_hq_256', transform=transform)\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True, num_workers=4, drop_last=True)\n",
    "\n",
    "    return dataloader\n",
    "\n",
    "layer_1_dataloader = get_dataloader(image_size=4, batch_size=batch_size)\n",
    "layer_2_dataloader = get_dataloader(image_size=8, batch_size=batch_size)\n",
    "layer_3_dataloader = get_dataloader(image_size=16, batch_size=batch_size)\n",
    "layer_4_dataloader = get_dataloader(image_size=32, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e5bab95c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test this reshape vs traditional reshape\n",
    "class Reshape(nn.Module):\n",
    "    def __init__(self, *args):\n",
    "        super().__init__()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "263d2338",
   "metadata": {},
   "outputs": [],
   "source": [
    "class G_ConvBlock(nn.Module):\n",
    "    def __init__(\n",
    "        self, \n",
    "        in_c, \n",
    "        out_c, \n",
    "        ksize1, \n",
    "        padding, \n",
    "        ksize2=None, \n",
    "        stride=None, \n",
    "        use_pn=True, \n",
    "        use_fc=False\n",
    "    ):\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        self.layers = []\n",
    "        \n",
    "        # Normalize input\n",
    "        layers.append(PixelNorm())\n",
    "        \n",
    "        if ksize2 = None:\n",
    "            ksize2 = ksize1\n",
    "        \n",
    "        if use_fc:\n",
    "            # Fully connected layer for input\n",
    "            fc_out = in_c * ksize1 * ksize2\n",
    "            layers.append(nn.Linear(in_features=512, out_features=fc_out))\n",
    "            layers.append(nn.Reshape(-1, out_c, ksize1, ksize2))\n",
    "            layers.append(nn.LeakyReLU(0.2))\n",
    "            layers.append(PixelNorm)\n",
    "            \n",
    "            # Conv 3x3 layer\n",
    "            layers.append\n",
    "            \n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpu_use",
   "language": "python",
   "name": "gpu_use"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
