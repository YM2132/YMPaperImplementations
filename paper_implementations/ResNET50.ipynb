{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "5048bdd2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-17T19:23:19.146800Z",
     "start_time": "2024-02-17T19:23:19.142724Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "\n",
    "import torch.nn as nn\n",
    "\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "11c080b0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-17T19:20:07.828605Z",
     "start_time": "2024-02-17T19:20:07.628386Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "    x = torch.ones(1, device=device)\n",
    "    print (x)\n",
    "    \n",
    "elif torch.backends.cuda.is_built():\n",
    "    device = torch.device(\"cuda\")\n",
    "    x = torch.ones(1, device=device)\n",
    "    print (x)\n",
    "    \n",
    "else:\n",
    "    print (\"MPS device not found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "676bca662cf9c8a1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-16T14:47:52.727940Z",
     "start_time": "2024-02-16T14:47:52.723779Z"
    }
   },
   "outputs": [],
   "source": [
    "# I create a residual block which will be reused by the model class\n",
    "# The residual block may change size depending on the size of the network, I will begin with the ResNET34 network and then move to the ResNET50\n",
    "\n",
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(\n",
    "            \n",
    "        )\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "4e55d063f1487c44",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-16T18:41:55.156172Z",
     "start_time": "2024-02-16T18:41:55.137786Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 64, 112, 112])\n",
      "torch.Size([1, 64, 56, 56])\n",
      "torch.Size([1, 64, 56, 56])\n",
      "torch.Size([1, 128, 28, 28])\n"
     ]
    }
   ],
   "source": [
    "dummy1 = torch.randn(1, 3, 224, 224)  # For example, a single 224x224 RGB image\n",
    "dummy1= dummy1.to(device)\n",
    "\n",
    "conv1 = nn.Conv2d(in_channels=3, out_channels=64, kernel_size=(7,7), stride=2, padding=3, device=device)\n",
    "dummy1 = conv1(dummy1)\n",
    "print(dummy1.shape)\n",
    "\n",
    "mp1 = nn.MaxPool2d(kernel_size=(3,3), stride=2, padding=1)\n",
    "dummy1 = mp1(dummy1)\n",
    "print(dummy1.shape)\n",
    "\n",
    "conv2 = nn.Conv2d(in_channels=64, out_channels=64, kernel_size=(3,3), stride=1, padding=1, device=device)\n",
    "dummy1 = conv2(dummy1)\n",
    "print(dummy1.shape)\n",
    "\n",
    "conv3 = nn.Conv2d(in_channels=64, out_channels=128, kernel_size=(3,3), stride=2, padding=1, device=device)\n",
    "dummy1_change_dim = conv3(dummy1)\n",
    "print(dummy1_change_dim.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "da729ca2be720e6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-16T18:42:30.388865Z",
     "start_time": "2024-02-16T18:42:30.382776Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 64, 56, 56])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dummy1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "2bc59fbbd82d1c7f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-16T18:44:27.006162Z",
     "start_time": "2024-02-16T18:44:26.857038Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 128, 28, 28])\n"
     ]
    }
   ],
   "source": [
    "conv_change_dim = nn.Conv2d(in_channels=64, out_channels=128, kernel_size=(1,1), stride=2, device=device)\n",
    "dummy1_change = conv_change_dim(dummy1)\n",
    "print(dummy1_change.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "de5ed9288e2d4ffb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-16T18:44:46.084793Z",
     "start_time": "2024-02-16T18:44:46.011670Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[ 0.1504,  0.3778,  0.1248,  ...,  0.4215,  0.3730,  0.2714],\n",
       "          [ 0.2804,  0.2868,  0.9122,  ...,  0.5940,  0.4803,  0.7434],\n",
       "          [ 0.4763,  0.7639,  0.6916,  ...,  0.5905,  0.5021,  0.5044],\n",
       "          ...,\n",
       "          [ 0.2601,  0.6739,  0.8382,  ...,  0.6568,  0.6527,  0.6523],\n",
       "          [ 0.2464,  0.5472,  0.8069,  ...,  0.5932,  0.7632,  0.6078],\n",
       "          [ 0.2615,  0.3440,  0.8023,  ...,  0.4130,  0.5459,  0.7129]],\n",
       "\n",
       "         [[-0.1580, -0.1041, -0.1342,  ..., -0.0392, -0.2626,  0.0531],\n",
       "          [-0.4320, -0.4274, -0.1070,  ..., -0.4674, -0.1672, -0.3652],\n",
       "          [-0.3088, -0.1924, -0.2040,  ...,  0.1007, -0.0948, -0.1875],\n",
       "          ...,\n",
       "          [-0.5458, -0.3418, -0.3320,  ..., -0.6077,  0.0449, -0.0755],\n",
       "          [-0.2638, -0.2043, -0.0681,  ..., -0.0342, -0.2294,  0.0404],\n",
       "          [-0.4018, -0.4768, -0.0253,  ..., -0.3825, -0.4072, -0.4644]],\n",
       "\n",
       "         [[ 0.0916,  0.0493, -0.1437,  ..., -0.1328, -0.0155,  0.0150],\n",
       "          [-0.1602, -0.4761, -0.7097,  ..., -0.1611, -0.6389, -0.3013],\n",
       "          [-0.0844, -0.5087, -0.3664,  ..., -0.4191, -0.4411, -0.5634],\n",
       "          ...,\n",
       "          [-0.0344, -0.6390, -0.3388,  ..., -0.3347, -0.2475, -0.4933],\n",
       "          [-0.2289, -0.1929, -0.4156,  ..., -0.3446, -0.2820, -0.4548],\n",
       "          [-0.1274, -0.3394, -0.3373,  ..., -0.5062, -0.6518, -0.6268]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[ 0.1160,  0.0525, -0.3543,  ..., -0.0860, -0.0986, -0.2141],\n",
       "          [-0.0585, -0.3219, -0.2965,  ..., -0.1502, -0.2009, -0.1860],\n",
       "          [ 0.0785,  0.0985,  0.1332,  ..., -0.4217, -0.1496, -0.1899],\n",
       "          ...,\n",
       "          [ 0.1635,  0.0293, -0.0997,  ..., -0.1597, -0.1513, -0.1876],\n",
       "          [ 0.1298, -0.1802,  0.0424,  ..., -0.1462, -0.1191, -0.2593],\n",
       "          [-0.0610, -0.0880, -0.0193,  ..., -0.0106, -0.2779, -0.2776]],\n",
       "\n",
       "         [[ 0.3387,  0.2007,  0.2325,  ...,  0.3874,  0.1748,  0.0033],\n",
       "          [ 0.4138,  0.1477,  0.5168,  ...,  0.2921,  0.6689, -0.1420],\n",
       "          [ 0.4172,  0.2686,  0.4307,  ...,  0.2204,  0.4622, -0.0476],\n",
       "          ...,\n",
       "          [ 0.6152,  0.1855,  0.0632,  ...,  0.2157,  0.3476,  0.2204],\n",
       "          [ 0.5003,  0.2616,  0.3036,  ...,  0.1421,  0.6426,  0.1629],\n",
       "          [ 0.4092,  0.0767,  0.1108,  ...,  0.1828,  0.2112,  0.0121]],\n",
       "\n",
       "         [[ 0.0474,  0.2980,  0.2211,  ...,  0.3091,  0.4778,  0.5353],\n",
       "          [ 0.2776,  0.2762,  0.2837,  ...,  0.1407,  0.2557,  0.2792],\n",
       "          [ 0.3572,  0.3655,  0.4874,  ...,  0.0867,  0.2753,  0.3323],\n",
       "          ...,\n",
       "          [ 0.3352,  0.3164,  0.5834,  ...,  0.5479,  0.5409,  0.3053],\n",
       "          [ 0.3822,  0.3333,  0.4105,  ...,  0.3407,  0.3006,  0.4580],\n",
       "          [ 0.4943,  0.1631,  0.6076,  ...,  0.5993,  0.7197,  0.4076]]]],\n",
       "       device='mps:0', grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dummy1_change + dummy1_change_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "37eeda6a16bb7619",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-17T19:20:13.357841Z",
     "start_time": "2024-02-17T19:20:13.204854Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ResNet18(\n",
       "  (conv0): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "  (bn0): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu0): ReLU()\n",
       "  (maxpool0): MaxPool2d(kernel_size=(3, 3), stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  (conv1_1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  (bn1_1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu1_1): ReLU()\n",
       "  (conv1_2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  (bn1_2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu1_2): ReLU()\n",
       "  (conv1_3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  (bn1_3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu1_3): ReLU()\n",
       "  (conv1_4): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  (bn1_4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu1_4): ReLU()\n",
       "  (conv2_1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "  (bn2_1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu2_1): ReLU()\n",
       "  (conv2_2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  (bn2_2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu2_2): ReLU()\n",
       "  (dim_match_conv1): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2))\n",
       "  (conv2_3): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  (bn2_3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu2_3): ReLU()\n",
       "  (conv2_4): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  (bn2_4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu2_4): ReLU()\n",
       "  (conv3_1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "  (bn3_1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu3_1): ReLU()\n",
       "  (conv3_2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  (bn3_2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu3_2): ReLU()\n",
       "  (dim_match_conv2): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2))\n",
       "  (conv3_3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  (bn3_3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu3_3): ReLU()\n",
       "  (conv3_4): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  (bn3_4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu3_4): ReLU()\n",
       "  (conv4_1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "  (bn4_1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu4_1): ReLU()\n",
       "  (conv4_2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  (bn4_2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu4_2): ReLU()\n",
       "  (dim_match_conv3): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2))\n",
       "  (conv4_3): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  (bn4_3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu4_3): ReLU()\n",
       "  (conv4_4): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  (bn4_4): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu4_4): ReLU()\n",
       "  (global_avg_pool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "  (fc1): Linear(in_features=512, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class ResNet18(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        # The first 7x7 conv layer\n",
    "        self.conv0 = nn.Conv2d(\n",
    "            in_channels=3, out_channels=64, \n",
    "            kernel_size=(7, 7),\n",
    "            stride=2, padding=3, bias=False,\n",
    "        )\n",
    "        self.bn0 = nn.BatchNorm2d(num_features=64)\n",
    "        self.relu0 = nn.ReLU()\n",
    "        self.maxpool0 = nn.MaxPool2d(\n",
    "            kernel_size=(3, 3), stride=2, padding=1,\n",
    "        )\n",
    "        \n",
    "        # First part of first conv block\n",
    "        self.conv1_1 = nn.Conv2d(\n",
    "            in_channels=64, out_channels=64,\n",
    "            kernel_size=(3,3),\n",
    "            stride=1, padding=1, bias=False,\n",
    "        )\n",
    "        self.bn1_1 = nn.BatchNorm2d(num_features=64)\n",
    "        self.relu1_1 = nn.ReLU()\n",
    "        \n",
    "        self.conv1_2 = nn.Conv2d(\n",
    "            in_channels=64, out_channels=64,\n",
    "            kernel_size=(3,3),\n",
    "            stride=1, padding=1, bias=False,\n",
    "        )\n",
    "        self.bn1_2 = nn.BatchNorm2d(num_features=64)\n",
    "        self.relu1_2 = nn.ReLU()\n",
    "        # Second part of first conv block\n",
    "        self.conv1_3 = nn.Conv2d(\n",
    "            in_channels=64, out_channels=64,\n",
    "            kernel_size=(3,3),\n",
    "            stride=1, padding=1, bias=False,\n",
    "        )\n",
    "        self.bn1_3 = nn.BatchNorm2d(num_features=64)\n",
    "        self.relu1_3 = nn.ReLU()\n",
    "        \n",
    "        self.conv1_4 = nn.Conv2d(\n",
    "            in_channels=64, out_channels=64,\n",
    "            kernel_size=(3,3),\n",
    "            stride=1, padding=1, bias=False,\n",
    "        )\n",
    "        self.bn1_4 = nn.BatchNorm2d(num_features=64)\n",
    "        self.relu1_4 = nn.ReLU()\n",
    "        \n",
    "        # First part of second conv block, this layer changes features to 128\n",
    "        self.conv2_1 = nn.Conv2d(\n",
    "            in_channels=64, out_channels=128,\n",
    "            kernel_size=(3,3),\n",
    "            stride=2, padding=1, bias=False,\n",
    "        )\n",
    "        self.bn2_1 = nn.BatchNorm2d(num_features=128)\n",
    "        self.relu2_1 = nn.ReLU()\n",
    "        \n",
    "        self.conv2_2 = nn.Conv2d(\n",
    "            in_channels=128, out_channels=128,\n",
    "            kernel_size=(3,3),\n",
    "            stride=1, padding=1, bias=False,\n",
    "        )\n",
    "        self.bn2_2 = nn.BatchNorm2d(num_features=128)\n",
    "        self.relu2_2 = nn.ReLU()\n",
    "        \n",
    "        self.dim_match_conv1 = nn.Conv2d(\n",
    "            in_channels=64, out_channels=128, \n",
    "            kernel_size=(1,1),\n",
    "            stride=2, \n",
    "        )\n",
    "        # Second part of second conv block\n",
    "        self.conv2_3 = nn.Conv2d(\n",
    "            in_channels=128, out_channels=128,\n",
    "            kernel_size=(3,3),\n",
    "            stride=1, padding=1, bias=False,\n",
    "        )\n",
    "        self.bn2_3 = nn.BatchNorm2d(num_features=128)\n",
    "        self.relu2_3 = nn.ReLU()\n",
    "        \n",
    "        self.conv2_4 = nn.Conv2d(\n",
    "            in_channels=128, out_channels=128,\n",
    "            kernel_size=(3,3),\n",
    "            stride=1, padding=1, bias=False,\n",
    "        )\n",
    "        self.bn2_4 = nn.BatchNorm2d(num_features=128)\n",
    "        self.relu2_4 = nn.ReLU()\n",
    "        \n",
    "        # First part of third conv block\n",
    "        self.conv3_1 = nn.Conv2d(\n",
    "            in_channels=128, out_channels=256,\n",
    "            kernel_size=(3,3),\n",
    "            stride=2, padding=1, bias=False,\n",
    "        )\n",
    "        self.bn3_1 = nn.BatchNorm2d(num_features=256)\n",
    "        self.relu3_1 = nn.ReLU()\n",
    "        \n",
    "        self.conv3_2 = nn.Conv2d(\n",
    "            in_channels=256, out_channels=256,\n",
    "            kernel_size=(3,3),\n",
    "            stride=1, padding=1, bias=False,\n",
    "        )\n",
    "        self.bn3_2 = nn.BatchNorm2d(num_features=256)\n",
    "        self.relu3_2 = nn.ReLU()\n",
    "        \n",
    "        self.dim_match_conv2 = nn.Conv2d(\n",
    "            in_channels=128, out_channels=256, \n",
    "            kernel_size=(1,1),\n",
    "            stride=2, \n",
    "        )\n",
    "        # Second part of third conv block\n",
    "        self.conv3_3 = nn.Conv2d(\n",
    "            in_channels=256, out_channels=256,\n",
    "            kernel_size=(3,3),\n",
    "            stride=1, padding=1, bias=False,\n",
    "        )\n",
    "        self.bn3_3 = nn.BatchNorm2d(num_features=256)\n",
    "        self.relu3_3 = nn.ReLU()\n",
    "        \n",
    "        self.conv3_4 = nn.Conv2d(\n",
    "            in_channels=256, out_channels=256,\n",
    "            kernel_size=(3,3),\n",
    "            stride=1, padding=1, bias=False,\n",
    "        )\n",
    "        self.bn3_4 = nn.BatchNorm2d(num_features=256)\n",
    "        self.relu3_4 = nn.ReLU()\n",
    "        \n",
    "        # First part of fourth conv block\n",
    "        self.conv4_1 = nn.Conv2d(\n",
    "            in_channels=256, out_channels=512,\n",
    "            kernel_size=(3,3),\n",
    "            stride=2, padding=1, bias=False,\n",
    "        )\n",
    "        self.bn4_1 = nn.BatchNorm2d(num_features=512)\n",
    "        self.relu4_1 = nn.ReLU()\n",
    "        \n",
    "        self.conv4_2 = nn.Conv2d(\n",
    "            in_channels=512, out_channels=512,\n",
    "            kernel_size=(3,3),\n",
    "            stride=1, padding=1, bias=False,\n",
    "        )\n",
    "        self.bn4_2 = nn.BatchNorm2d(num_features=512)\n",
    "        self.relu4_2 = nn.ReLU()\n",
    "        \n",
    "        self.dim_match_conv3 = nn.Conv2d(\n",
    "            in_channels=256, out_channels=512, \n",
    "            kernel_size=(1,1),\n",
    "            stride=2, \n",
    "        )\n",
    "        # Second part of fourth conv block\n",
    "        self.conv4_3 = nn.Conv2d(\n",
    "            in_channels=512, out_channels=512,\n",
    "            kernel_size=(3,3),\n",
    "            stride=1, padding=1, bias=False,\n",
    "        )\n",
    "        self.bn4_3 = nn.BatchNorm2d(num_features=512)\n",
    "        self.relu4_3 = nn.ReLU()\n",
    "        \n",
    "        self.conv4_4 = nn.Conv2d(\n",
    "            in_channels=512, out_channels=512,\n",
    "            kernel_size=(3,3),\n",
    "            stride=1, padding=1, bias=False,\n",
    "        )\n",
    "        self.bn4_4 = nn.BatchNorm2d(num_features=512)\n",
    "        self.relu4_4 = nn.ReLU()\n",
    "        \n",
    "        # Avg pool out put and pass through a FC connected layer which has 10 outputs for CIFAR10 \n",
    "        # (Change to 1000 for ImageNet)\n",
    "        self.global_avg_pool = nn.AdaptiveAvgPool2d(\n",
    "            output_size=(1,1)\n",
    "        )\n",
    "        \n",
    "        # Final FC layer\n",
    "        self.fc1 = nn.Linear(\n",
    "            in_features=512, out_features=10\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "           \n",
    "        x = self.conv0(x)\n",
    "        x = self.bn0(x)\n",
    "        x = self.relu0(x)\n",
    "        x = self.maxpool0(x)  \n",
    "        res_x = x.detach().clone()  # The x to be passed forward to the +2 layer\n",
    "        #print(f'dim after init conv: {x.shape}')\n",
    "        \n",
    "        # First residual block of first conv block\n",
    "        x = self.conv1_1(x)\n",
    "        x = self.bn1_1(x)\n",
    "        x = self.relu1_1(x)\n",
    "        x = self.conv1_2(x)\n",
    "        x = self.bn1_2(x)\n",
    "        x = self.relu1_2(x)\n",
    "        x = x + res_x\n",
    "        # res_x = x.detach().clone()  This might be the wrong approach for making res_x\n",
    "        res_x = x\n",
    "        #print(f'dim after first res block in first conv block {x.shape}')\n",
    "        \n",
    "        # Second residual block of first conv block\n",
    "        x = self.conv1_3(x)\n",
    "        x = self.bn1_3(x)\n",
    "        x = self.relu1_3(x)\n",
    "        x = self.conv1_3(x)\n",
    "        x = self.bn1_3(x)\n",
    "        x = self.relu1_3(x)\n",
    "        x = x + res_x\n",
    "        #res_x = x.detach().clone()\n",
    "        res_x = x\n",
    "        #print(f'dim after second res block in first conv block {x.shape}')\n",
    "        \n",
    "        # First residual block of second conv block\n",
    "        x = self.conv2_1(x)\n",
    "        x = self.bn2_1(x)\n",
    "        x = self.relu2_1(x)\n",
    "        x = self.conv2_2(x)\n",
    "        x = self.bn2_2(x)\n",
    "        x = self.relu2_2(x)\n",
    "        \n",
    "        # Must perform a (1x1) conv on res_x to make the dimensions match\n",
    "        res_x = self.dim_match_conv1(res_x)\n",
    "        #print(f'dim res_x after 1x1 conv: {res_x.shape}')\n",
    "        x = x + res_x\n",
    "        #res_x = x.detach().clone()\n",
    "        res_x = x\n",
    "        #print(f'dim after first res block in second conv block {x.shape}')\n",
    "        \n",
    "        # Second residual block of second conv block\n",
    "        x = self.conv2_3(x)\n",
    "        x = self.bn2_3(x)\n",
    "        x = self.relu2_3(x)\n",
    "        x = self.conv2_4(x)\n",
    "        x = self.bn2_4(x)\n",
    "        x = self.relu2_4(x)\n",
    "        x = x + res_x\n",
    "        #res_x = x.detach().clone()\n",
    "        res_x = x\n",
    "        #print(f'dim after second res block in second conv block {x.shape}')\n",
    "        \n",
    "        # First residual block of third conv block\n",
    "        x = self.conv3_1(x)\n",
    "        x = self.bn3_1(x)\n",
    "        x = self.relu3_1(x)\n",
    "        x = self.conv3_2(x)\n",
    "        x = self.bn3_2(x)\n",
    "        x = self.relu3_2(x)\n",
    "        \n",
    "        # Must perform a (1x1) conv on res_x to make the dimensions match\n",
    "        res_x = self.dim_match_conv2(res_x)\n",
    "        #print(f'dim res_x after 1x1 conv: {res_x.shape}')\n",
    "        x = x + res_x\n",
    "        #res_x = x.detach().clone()\n",
    "        res_x = x\n",
    "        #print(f'dim after first res block in third conv block {x.shape}')\n",
    "        \n",
    "        # Second residual block of third conv block\n",
    "        x = self.conv3_3(x)\n",
    "        x = self.bn3_3(x)\n",
    "        x = self.relu3_3(x)\n",
    "        x = self.conv3_4(x)\n",
    "        x = self.bn3_4(x)\n",
    "        x = self.relu3_4(x)\n",
    "        x = x + res_x\n",
    "        #res_x = x.detach().clone()\n",
    "        res_x = x\n",
    "        #print(f'dim after second res block in third conv block {x.shape}')\n",
    "        \n",
    "        # First residual block of fourth conv block\n",
    "        x = self.conv4_1(x)\n",
    "        x = self.bn4_1(x)\n",
    "        x = self.relu4_1(x)\n",
    "        x = self.conv4_2(x)\n",
    "        x = self.bn4_2(x)\n",
    "        x = self.relu4_2(x)\n",
    "        \n",
    "        # Must perform a (1x1) conv on res_x to make the dimensions match\n",
    "        res_x = self.dim_match_conv3(res_x)\n",
    "        #print(f'dim res_x after 1x1 conv: {res_x.shape}')\n",
    "        x = x + res_x\n",
    "        #res_x = x.detach().clone()\n",
    "        res_x = x\n",
    "        #print(f'dim after first res block in fourth conv block {x.shape}')\n",
    "        \n",
    "        # Second residual block of third conv block\n",
    "        x = self.conv4_3(x)\n",
    "        x = self.bn4_3(x)\n",
    "        x = self.relu4_3(x)\n",
    "        x = self.conv4_4(x)\n",
    "        x = self.bn4_4(x)\n",
    "        x = self.relu4_4(x)\n",
    "        x = x + res_x\n",
    "        # res_x = x.detach().clone()\n",
    "        #print(f'dim after second res block in fourth conv block {x.shape}')\n",
    "        \n",
    "        x = self.global_avg_pool(x)\n",
    "        #print(f'dim after global avg pool: {x.shape}')\n",
    "        # Flatten after pooling to make output into a vector ready for FC layer\n",
    "        x = torch.flatten(x, start_dim=1)\n",
    "        #print(f'dim after flatten: {x.shape}')\n",
    "        \n",
    "        # Final fc layer\n",
    "        x = self.fc1(x)\n",
    "        #print(f'Final output dim: {x.shape}')\n",
    "        \n",
    "        return x\n",
    "\n",
    "def kaiming_uniform_init(net):\n",
    "    if isinstance(net, nn.Linear) or isinstance(net, nn.Conv2d):\n",
    "        nn.init.kaiming_uniform_(net.weight)\n",
    "        if net.bias is not None:\n",
    "            nn.init.constant_(net.bias, 0)  \n",
    "\n",
    "model = ResNet18()\n",
    "model.apply(kaiming_uniform_init)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "74a707f505d08b97",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-17T19:20:15.116624Z",
     "start_time": "2024-02-17T19:20:14.876758Z"
    }
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Given groups=1, weight of size [64, 3, 7, 7], expected input[1, 1, 32, 32] to have 3 channels, but got 1 channels instead",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[48], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m dummy \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrandn(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m32\u001b[39m, \u001b[38;5;241m32\u001b[39m)  \u001b[38;5;66;03m# For example, a single 224x224 RGB image\u001b[39;00m\n\u001b[1;32m      2\u001b[0m dummy \u001b[38;5;241m=\u001b[39m dummy\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m----> 4\u001b[0m out \u001b[38;5;241m=\u001b[39m model(dummy)\n",
      "File \u001b[0;32m~/anaconda3/envs/gpu_use/lib/python3.11/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/gpu_use/lib/python3.11/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[47], line 180\u001b[0m, in \u001b[0;36mResNet18.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    178\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m--> 180\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv0(x)\n\u001b[1;32m    181\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbn0(x)\n\u001b[1;32m    182\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrelu0(x)\n",
      "File \u001b[0;32m~/anaconda3/envs/gpu_use/lib/python3.11/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/gpu_use/lib/python3.11/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/gpu_use/lib/python3.11/site-packages/torch/nn/modules/conv.py:460\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    459\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 460\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_conv_forward(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias)\n",
      "File \u001b[0;32m~/anaconda3/envs/gpu_use/lib/python3.11/site-packages/torch/nn/modules/conv.py:456\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    452\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mzeros\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m    453\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv2d(F\u001b[38;5;241m.\u001b[39mpad(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode),\n\u001b[1;32m    454\u001b[0m                     weight, bias, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstride,\n\u001b[1;32m    455\u001b[0m                     _pair(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdilation, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups)\n\u001b[0;32m--> 456\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv2d(\u001b[38;5;28minput\u001b[39m, weight, bias, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstride,\n\u001b[1;32m    457\u001b[0m                 \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdilation, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Given groups=1, weight of size [64, 3, 7, 7], expected input[1, 1, 32, 32] to have 3 channels, but got 1 channels instead"
     ]
    }
   ],
   "source": [
    "dummy = torch.randn(1, 1, 32, 32)  # For example, a single 224x224 RGB image\n",
    "dummy = dummy.to(device)\n",
    "\n",
    "out = model(dummy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "9341fca0a28e54b6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-17T19:20:48.753706Z",
     "start_time": "2024-02-17T19:20:47.103368Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "from torchvision.datasets import CIFAR10\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.4914, 0.48227 ,0.4465], std=[0.2470, 0.2435, 0.2616]), #Normalize using mean and std dev of cifar10\n",
    "])\n",
    "\n",
    "# Load datasets\n",
    "train_dataset = CIFAR10(root='./CIFAR', train=True, download=True, transform=transform)\n",
    "test_dataset = CIFAR10(root='./CIFAR', train=False, download=True, transform=transform)\n",
    "\n",
    "# Create data loaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True) # 256 batch size for imagenet\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "e3938be18d209d57",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-17T19:24:59.149455Z",
     "start_time": "2024-02-17T19:24:59.115391Z"
    }
   },
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.0001, weight_decay=0.0001, momentum=0.9)\n",
    "\n",
    "# Define learning rate scheduler\n",
    "scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=5, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "cf8db151d34a7023",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1] training loss: 1.964\n",
      "Epoch [1] validation loss: 1.989, accuracy: 30.27%\n",
      "Epoch [2] training loss: 1.609\n",
      "Epoch [2] validation loss: 1.932, accuracy: 33.76%\n",
      "Epoch [3] training loss: 1.495\n",
      "Epoch [3] validation loss: 1.686, accuracy: 39.81%\n",
      "Epoch [4] training loss: 1.419\n",
      "Epoch [4] validation loss: 1.633, accuracy: 43.59%\n",
      "Epoch [5] training loss: 1.363\n",
      "Epoch [5] validation loss: 1.585, accuracy: 43.74%\n",
      "Epoch [6] training loss: 1.320\n",
      "Epoch [6] validation loss: 1.641, accuracy: 41.97%\n",
      "Epoch [7] training loss: 1.282\n",
      "Epoch [7] validation loss: 1.621, accuracy: 42.98%\n",
      "Epoch [8] training loss: 1.241\n",
      "Epoch [8] validation loss: 1.636, accuracy: 43.36%\n",
      "Epoch [9] training loss: 1.215\n",
      "Epoch [9] validation loss: 1.758, accuracy: 39.69%\n",
      "Epoch [10] training loss: 1.188\n",
      "Epoch [10] validation loss: 1.754, accuracy: 40.06%\n",
      "Epoch [11] training loss: 1.165\n",
      "Epoch [11] validation loss: 1.629, accuracy: 43.34%\n",
      "Epoch [12] training loss: 1.106\n",
      "Epoch [12] validation loss: 1.643, accuracy: 42.88%\n",
      "Epoch [13] training loss: 1.101\n",
      "Epoch [13] validation loss: 1.692, accuracy: 41.97%\n",
      "Epoch [14] training loss: 1.099\n",
      "Epoch [14] validation loss: 1.683, accuracy: 41.79%\n",
      "Epoch [15] training loss: 1.096\n",
      "Epoch [15] validation loss: 1.677, accuracy: 42.04%\n",
      "Epoch [16] training loss: 1.091\n",
      "Epoch [16] validation loss: 1.675, accuracy: 42.27%\n",
      "Epoch [17] training loss: 1.092\n",
      "Epoch [17] validation loss: 1.647, accuracy: 43.13%\n",
      "Epoch [18] training loss: 1.086\n",
      "Epoch [18] validation loss: 1.684, accuracy: 42.01%\n",
      "Epoch [19] training loss: 1.086\n",
      "Epoch [19] validation loss: 1.647, accuracy: 42.88%\n",
      "Epoch [20] training loss: 1.084\n",
      "Epoch [20] validation loss: 1.698, accuracy: 41.82%\n",
      "Epoch [21] training loss: 1.084\n",
      "Epoch [21] validation loss: 1.646, accuracy: 42.67%\n",
      "Epoch [22] training loss: 1.085\n",
      "Epoch [22] validation loss: 1.693, accuracy: 41.89%\n",
      "Epoch [23] training loss: 1.084\n",
      "Epoch [23] validation loss: 1.686, accuracy: 41.80%\n",
      "Epoch [24] training loss: 1.083\n",
      "Epoch [24] validation loss: 1.709, accuracy: 41.28%\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[46], line 15\u001b[0m\n\u001b[1;32m     12\u001b[0m     loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m     13\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m---> 15\u001b[0m     running_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n\u001b[1;32m     17\u001b[0m train_loss \u001b[38;5;241m=\u001b[39m running_loss \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mlen\u001b[39m(train_loader)\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEpoch [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;250m \u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m] training loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.3f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for epoch in range(10000):\n",
    "    model.train()  # Set model to training mode\n",
    "    running_loss = 0.0\n",
    "    \n",
    "    for i, data in enumerate(train_loader, 0):\n",
    "        inputs, labels = data\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "    \n",
    "    train_loss = running_loss / len(train_loader)\n",
    "    print(f'Epoch [{epoch + 1}] training loss: {train_loss:.3f}')\n",
    "    \n",
    "    # Validation phase\n",
    "    model.eval()  # Set model to evaluation mode\n",
    "    val_running_loss = 0.0\n",
    "    val_correct = 0\n",
    "    val_total = 0\n",
    "    with torch.no_grad():\n",
    "        for data in test_loader:  # Assuming test_loader is used as a validation loader\n",
    "            inputs, labels = data\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            \n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            val_running_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            val_total += labels.size(0)\n",
    "            val_correct += (predicted == labels).sum().item()\n",
    "    \n",
    "    val_loss = val_running_loss / len(test_loader)\n",
    "    val_accuracy = 100 * val_correct / val_total\n",
    "    print(f'Epoch [{epoch + 1}] validation loss: {val_loss:.3f}, accuracy: {val_accuracy:.2f}%')\n",
    "    \n",
    "    # Update the LR scheduler with validation loss\n",
    "    scheduler.step(val_loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92e17623",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make batch size 128, didnt seem to help"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpu_use",
   "language": "python",
   "name": "gpu_use"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
