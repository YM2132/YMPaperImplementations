{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f27105b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "\n",
    "import torch.nn as nn\n",
    "\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "\n",
    "from torchinfo import summary\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6e910b93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "if torch.backends.cuda.is_built():\n",
    "    device = torch.device(\"cuda\")\n",
    "    x = torch.ones(1, device=device)\n",
    "    print (x)\n",
    "    \n",
    "else:\n",
    "    print (\"Cuda device not found.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5476898d",
   "metadata": {},
   "source": [
    "## First define base model C from table 1 and train on Cifar10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "280ca699",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                   Output Shape              Param #\n",
       "==========================================================================================\n",
       "base_c                                   [1, 10]                   --\n",
       "├─Dropout2d: 1-1                         [1, 3, 32, 32]            --\n",
       "├─Conv2d: 1-2                            [1, 96, 34, 34]           2,688\n",
       "├─ReLU: 1-3                              [1, 96, 34, 34]           --\n",
       "├─Conv2d: 1-4                            [1, 96, 36, 36]           83,040\n",
       "├─ReLU: 1-5                              [1, 96, 36, 36]           --\n",
       "├─MaxPool2d: 1-6                         [1, 96, 17, 17]           --\n",
       "├─Dropout2d: 1-7                         [1, 96, 17, 17]           --\n",
       "├─Conv2d: 1-8                            [1, 192, 17, 17]          166,080\n",
       "├─ReLU: 1-9                              [1, 192, 17, 17]          --\n",
       "├─Conv2d: 1-10                           [1, 192, 17, 17]          331,968\n",
       "├─ReLU: 1-11                             [1, 192, 17, 17]          --\n",
       "├─MaxPool2d: 1-12                        [1, 192, 8, 8]            --\n",
       "├─Dropout2d: 1-13                        [1, 192, 8, 8]            --\n",
       "├─Conv2d: 1-14                           [1, 192, 6, 6]            331,968\n",
       "├─ReLU: 1-15                             [1, 192, 6, 6]            --\n",
       "├─Conv2d: 1-16                           [1, 192, 6, 6]            37,056\n",
       "├─ReLU: 1-17                             [1, 192, 6, 6]            --\n",
       "├─Conv2d: 1-18                           [1, 10, 6, 6]             1,930\n",
       "├─ReLU: 1-19                             [1, 10, 6, 6]             --\n",
       "├─AvgPool2d: 1-20                        [1, 10, 1, 1]             --\n",
       "==========================================================================================\n",
       "Total params: 954,730\n",
       "Trainable params: 954,730\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (Units.MEGABYTES): 268.02\n",
       "==========================================================================================\n",
       "Input size (MB): 0.01\n",
       "Forward/backward pass size (MB): 2.88\n",
       "Params size (MB): 3.82\n",
       "Estimated Total Size (MB): 6.72\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class base_c(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.dropout1 = nn.Dropout2d(p=0.2)\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(\n",
    "            in_channels=3, out_channels=96,\n",
    "            kernel_size=(3,3),\n",
    "            stride=1, padding=2,\n",
    "        )\n",
    "        self.relu1 = nn.ReLU()\n",
    "        \n",
    "        self.conv2 = nn.Conv2d(\n",
    "            in_channels=96, out_channels=96,\n",
    "            kernel_size=(3,3),\n",
    "            stride=1, padding=2,\n",
    "        )\n",
    "        self.relu2 = nn.ReLU()\n",
    "        \n",
    "        self.mp1 = nn.MaxPool2d(\n",
    "            stride=2, kernel_size=(3,3), padding=0,\n",
    "        )\n",
    "        self.dropout2 = nn.Dropout2d(p=0.5)\n",
    "        \n",
    "        self.conv3 = nn.Conv2d(\n",
    "            in_channels=96, out_channels=192,\n",
    "            kernel_size=(3,3),\n",
    "            stride=1, padding=1,\n",
    "        )\n",
    "        self.relu3 = nn.ReLU()\n",
    "        \n",
    "        self.conv4 = nn.Conv2d(\n",
    "            in_channels=192, out_channels=192,\n",
    "            kernel_size=(3,3),\n",
    "            stride=1, padding=1,\n",
    "        )\n",
    "        self.relu4 = nn.ReLU()\n",
    "        \n",
    "        self.mp2 = nn.MaxPool2d(\n",
    "            stride=2, kernel_size=(3,3), padding=0,\n",
    "        )\n",
    "        self.dropout3 = nn.Dropout2d(p=0.5)\n",
    "        \n",
    "        self.conv5 = nn.Conv2d(\n",
    "            in_channels=192, out_channels=192,\n",
    "            kernel_size=(3,3),\n",
    "            stride=1, padding=0,\n",
    "        )\n",
    "        self.relu5 = nn.ReLU()\n",
    "        \n",
    "        self.conv6 = nn.Conv2d(\n",
    "            in_channels=192, out_channels=192,\n",
    "            kernel_size=(1,1),\n",
    "            stride=1, padding=0,\n",
    "        )\n",
    "        self.relu6 = nn.ReLU()\n",
    "        \n",
    "        self.conv7 = nn.Conv2d(\n",
    "            in_channels=192, out_channels=10,\n",
    "            kernel_size=(1,1),\n",
    "            stride=1, padding=0,\n",
    "        )\n",
    "        self.relu7 = nn.ReLU()\n",
    "        \n",
    "        self.avg_pool = nn.AvgPool2d(\n",
    "            kernel_size=(6,6)\n",
    "        )\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.dropout1(x)\n",
    "        #print(f'Shape after dropout1: {x.shape}')\n",
    "        x = self.conv1(x)\n",
    "        x = self.relu1(x)\n",
    "        #print(f'Shape after conv1: {x.shape}')\n",
    "        x = self.conv2(x)\n",
    "        x = self.relu2(x)\n",
    "        #print(f'Shape after conv2: {x.shape}')\n",
    "        x = self.mp1(x)\n",
    "        #print(f'Shape after mp1: {x.shape}')\n",
    "        x = self.dropout2(x)\n",
    "        #print(f'Shape after dropout2: {x.shape}')\n",
    "        x = self.conv3(x)\n",
    "        x = self.relu3(x)\n",
    "        #print(f'Shape after conv3: {x.shape}')\n",
    "        x = self.conv4(x)\n",
    "        x = self.relu4(x)\n",
    "        #print(f'Shape after conv4: {x.shape}')\n",
    "        x = self.mp2(x)\n",
    "        #print(f'Shape after mp2: {x.shape}')\n",
    "        x = self.dropout3(x)\n",
    "        #print(f'Shape after dropout3: {x.shape}')\n",
    "        x = self.conv5(x)\n",
    "        x = self.relu5(x)\n",
    "        #print(f'Shape after conv5: {x.shape}')\n",
    "        x = self.conv6(x)\n",
    "        x = self.relu6(x)\n",
    "        #print(f'Shape after conv6: {x.shape}')\n",
    "        x = self.conv7(x)\n",
    "        x = self.relu7(x)\n",
    "        #print(f'Shape after conv7: {x.shape}')\n",
    "        x = self.avg_pool(x)\n",
    "        #print(f'Shape after global pool layer: {x.shape}')\n",
    "        x = torch.flatten(x, start_dim=1)\n",
    "        #print(f'out shape: {x.shape}')\n",
    "        \n",
    "        return x\n",
    "\n",
    "def glorot_uniform_init(net):\n",
    "    if isinstance(net, nn.Conv2d):\n",
    "        nn.init.xavier_normal_(net.weight)\n",
    "        if net.bias is not None:\n",
    "            nn.init.constant_(net.bias, 0)\n",
    "\n",
    "base_model_c = base_c()\n",
    "base_model_c.apply(glorot_uniform_init)\n",
    "base_model_c = base_model_c.to(device)\n",
    "summary(base_model_c, input_size=(1, 3, 32, 32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59bd8e9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_in = torch.randn(1, 3, 32, 32)\n",
    "dummy_in = dummy_in.to(device)\n",
    "out = base_model_c(dummy_in)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86cc2aea",
   "metadata": {},
   "source": [
    "### Testing Base model on Cifar10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "371af742",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "from torchvision.datasets import CIFAR10\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    #transforms.Resize(224), # Not needed for All CNN paper\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.4914, 0.48227 ,0.4465], std=[0.2470, 0.2435, 0.2616]), \n",
    "])\n",
    "\n",
    "# Load datasets\n",
    "train_dataset = CIFAR10(root='./CIFAR', train=True, download=True, transform=transform)\n",
    "test_dataset = CIFAR10(root='./CIFAR', train=False, download=True, transform=transform)\n",
    "\n",
    "# Create data loaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True, num_workers=4) # 256 batch size for imagenet\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f163b55b",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(base_model_c.parameters(), lr=0.01, weight_decay=0.001, momentum=0.9)\n",
    "\n",
    "# Define learning rate scheduler\n",
    "scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a5bf3b95",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1] training loss: 2.040\n",
      "Epoch [1] validation loss: 1.806, accuracy: 30.75%\n",
      "LR: [0.01]\n",
      "Epoch [2] training loss: 1.771\n",
      "Epoch [2] validation loss: 1.702, accuracy: 37.38%\n",
      "LR: [0.01]\n",
      "Epoch [3] training loss: 1.647\n",
      "Epoch [3] validation loss: 1.475, accuracy: 45.32%\n",
      "LR: [0.01]\n",
      "Epoch [4] training loss: 1.550\n",
      "Epoch [4] validation loss: 1.356, accuracy: 52.38%\n",
      "LR: [0.01]\n",
      "Epoch [5] training loss: 1.445\n",
      "Epoch [5] validation loss: 1.211, accuracy: 57.62%\n",
      "LR: [0.01]\n",
      "Epoch [6] training loss: 1.361\n",
      "Epoch [6] validation loss: 1.199, accuracy: 57.69%\n",
      "LR: [0.01]\n",
      "Epoch [7] training loss: 1.290\n",
      "Epoch [7] validation loss: 1.118, accuracy: 60.94%\n",
      "LR: [0.01]\n",
      "Epoch [8] training loss: 1.215\n",
      "Epoch [8] validation loss: 1.013, accuracy: 64.95%\n",
      "LR: [0.01]\n",
      "Epoch [9] training loss: 1.158\n",
      "Epoch [9] validation loss: 0.943, accuracy: 66.74%\n",
      "LR: [0.01]\n",
      "Epoch [10] training loss: 1.118\n",
      "Epoch [10] validation loss: 1.031, accuracy: 62.83%\n",
      "LR: [0.01]\n",
      "Epoch [11] training loss: 1.070\n",
      "Epoch [11] validation loss: 0.891, accuracy: 69.08%\n",
      "LR: [0.01]\n",
      "Epoch [12] training loss: 1.030\n",
      "Epoch [12] validation loss: 0.860, accuracy: 69.85%\n",
      "LR: [0.01]\n",
      "Epoch [13] training loss: 0.990\n",
      "Epoch [13] validation loss: 0.794, accuracy: 72.94%\n",
      "LR: [0.01]\n",
      "Epoch [14] training loss: 0.966\n",
      "Epoch [14] validation loss: 0.801, accuracy: 72.97%\n",
      "LR: [0.01]\n",
      "Epoch [15] training loss: 0.930\n",
      "Epoch [15] validation loss: 0.797, accuracy: 72.47%\n",
      "LR: [0.01]\n",
      "Epoch [16] training loss: 0.910\n",
      "Epoch [16] validation loss: 0.744, accuracy: 74.57%\n",
      "LR: [0.01]\n",
      "Epoch [17] training loss: 0.886\n",
      "Epoch [17] validation loss: 0.774, accuracy: 73.58%\n",
      "LR: [0.01]\n",
      "Epoch [18] training loss: 0.863\n",
      "Epoch [18] validation loss: 0.739, accuracy: 74.55%\n",
      "LR: [0.01]\n",
      "Epoch [19] training loss: 0.841\n",
      "Epoch [19] validation loss: 0.703, accuracy: 76.24%\n",
      "LR: [0.01]\n",
      "Epoch [20] training loss: 0.831\n",
      "Epoch [20] validation loss: 0.704, accuracy: 76.60%\n",
      "LR: [0.01]\n",
      "Epoch [21] training loss: 0.808\n",
      "Epoch [21] validation loss: 0.640, accuracy: 78.33%\n",
      "LR: [0.01]\n",
      "Epoch [22] training loss: 0.804\n",
      "Epoch [22] validation loss: 0.684, accuracy: 76.98%\n",
      "LR: [0.01]\n",
      "Epoch [23] training loss: 0.784\n",
      "Epoch [23] validation loss: 0.650, accuracy: 77.53%\n",
      "LR: [0.01]\n",
      "Epoch [24] training loss: 0.773\n",
      "Epoch [24] validation loss: 0.628, accuracy: 78.62%\n",
      "LR: [0.01]\n",
      "Epoch [25] training loss: 0.761\n",
      "Epoch [25] validation loss: 0.633, accuracy: 79.16%\n",
      "LR: [0.01]\n",
      "Epoch [26] training loss: 0.756\n",
      "Epoch [26] validation loss: 0.641, accuracy: 78.04%\n",
      "LR: [0.01]\n",
      "Epoch [27] training loss: 0.742\n",
      "Epoch [27] validation loss: 0.594, accuracy: 80.01%\n",
      "LR: [0.01]\n",
      "Epoch [28] training loss: 0.731\n",
      "Epoch [28] validation loss: 0.612, accuracy: 79.29%\n",
      "LR: [0.01]\n",
      "Epoch [29] training loss: 0.727\n",
      "Epoch [29] validation loss: 0.594, accuracy: 79.87%\n",
      "LR: [0.01]\n",
      "Epoch [30] training loss: 0.721\n",
      "Epoch [30] validation loss: 0.565, accuracy: 81.44%\n",
      "LR: [0.01]\n",
      "Epoch [31] training loss: 0.712\n",
      "Epoch [31] validation loss: 0.563, accuracy: 81.25%\n",
      "LR: [0.01]\n",
      "Epoch [32] training loss: 0.704\n",
      "Epoch [32] validation loss: 0.638, accuracy: 77.90%\n",
      "LR: [0.01]\n",
      "Epoch [33] training loss: 0.699\n",
      "Epoch [33] validation loss: 0.581, accuracy: 80.07%\n",
      "LR: [0.01]\n",
      "Epoch [34] training loss: 0.695\n",
      "Epoch [34] validation loss: 0.605, accuracy: 79.39%\n",
      "LR: [0.01]\n",
      "Epoch [35] training loss: 0.686\n",
      "Epoch [35] validation loss: 0.567, accuracy: 80.82%\n",
      "LR: [0.001]\n",
      "Epoch [36] training loss: 0.583\n",
      "Epoch [36] validation loss: 0.508, accuracy: 83.18%\n",
      "LR: [0.001]\n",
      "Epoch [37] training loss: 0.548\n",
      "Epoch [37] validation loss: 0.489, accuracy: 83.60%\n",
      "LR: [0.001]\n",
      "Epoch [38] training loss: 0.540\n",
      "Epoch [38] validation loss: 0.493, accuracy: 83.48%\n",
      "LR: [0.001]\n",
      "Epoch [39] training loss: 0.533\n",
      "Epoch [39] validation loss: 0.488, accuracy: 83.49%\n",
      "LR: [0.001]\n",
      "Epoch [40] training loss: 0.527\n",
      "Epoch [40] validation loss: 0.480, accuracy: 83.79%\n",
      "LR: [0.001]\n",
      "Epoch [41] training loss: 0.518\n",
      "Epoch [41] validation loss: 0.475, accuracy: 83.97%\n",
      "LR: [0.001]\n",
      "Epoch [42] training loss: 0.517\n",
      "Epoch [42] validation loss: 0.478, accuracy: 84.11%\n",
      "LR: [0.001]\n",
      "Epoch [43] training loss: 0.513\n",
      "Epoch [43] validation loss: 0.468, accuracy: 84.33%\n",
      "LR: [0.001]\n",
      "Epoch [44] training loss: 0.513\n",
      "Epoch [44] validation loss: 0.463, accuracy: 84.29%\n",
      "LR: [0.001]\n",
      "Epoch [45] training loss: 0.506\n",
      "Epoch [45] validation loss: 0.465, accuracy: 84.21%\n",
      "LR: [0.001]\n",
      "Epoch [46] training loss: 0.503\n",
      "Epoch [46] validation loss: 0.463, accuracy: 84.42%\n",
      "LR: [0.001]\n",
      "Epoch [47] training loss: 0.505\n",
      "Epoch [47] validation loss: 0.469, accuracy: 84.32%\n",
      "LR: [0.001]\n",
      "Epoch [48] training loss: 0.500\n",
      "Epoch [48] validation loss: 0.463, accuracy: 84.35%\n",
      "LR: [0.001]\n",
      "Epoch [49] training loss: 0.494\n",
      "Epoch [49] validation loss: 0.461, accuracy: 84.54%\n",
      "LR: [0.001]\n",
      "Epoch [50] training loss: 0.494\n",
      "Epoch [50] validation loss: 0.462, accuracy: 84.56%\n",
      "LR: [0.001]\n",
      "Epoch [51] training loss: 0.496\n",
      "Epoch [51] validation loss: 0.456, accuracy: 84.69%\n",
      "LR: [0.001]\n",
      "Epoch [52] training loss: 0.493\n",
      "Epoch [52] validation loss: 0.474, accuracy: 83.97%\n",
      "LR: [0.001]\n",
      "Epoch [53] training loss: 0.489\n",
      "Epoch [53] validation loss: 0.468, accuracy: 84.28%\n",
      "LR: [0.001]\n",
      "Epoch [54] training loss: 0.484\n",
      "Epoch [54] validation loss: 0.444, accuracy: 85.20%\n",
      "LR: [0.001]\n",
      "Epoch [55] training loss: 0.484\n",
      "Epoch [55] validation loss: 0.455, accuracy: 84.72%\n",
      "LR: [0.001]\n",
      "Epoch [56] training loss: 0.483\n",
      "Epoch [56] validation loss: 0.459, accuracy: 84.73%\n",
      "LR: [0.001]\n",
      "Epoch [57] training loss: 0.481\n",
      "Epoch [57] validation loss: 0.450, accuracy: 84.74%\n",
      "LR: [0.001]\n",
      "Epoch [58] training loss: 0.481\n",
      "Epoch [58] validation loss: 0.472, accuracy: 84.14%\n",
      "LR: [0.0001]\n",
      "Epoch [59] training loss: 0.468\n",
      "Epoch [59] validation loss: 0.446, accuracy: 85.18%\n",
      "LR: [0.0001]\n",
      "Epoch [60] training loss: 0.462\n",
      "Epoch [60] validation loss: 0.445, accuracy: 85.14%\n",
      "LR: [0.0001]\n",
      "Epoch [61] training loss: 0.460\n",
      "Epoch [61] validation loss: 0.449, accuracy: 85.07%\n",
      "LR: [0.0001]\n",
      "Epoch [62] training loss: 0.461\n",
      "Epoch [62] validation loss: 0.445, accuracy: 85.25%\n",
      "LR: [1e-05]\n",
      "Epoch [63] training loss: 0.460\n",
      "Epoch [63] validation loss: 0.443, accuracy: 85.27%\n",
      "LR: [1e-05]\n",
      "Epoch [64] training loss: 0.457\n",
      "Epoch [64] validation loss: 0.443, accuracy: 85.25%\n",
      "LR: [1e-05]\n",
      "Epoch [65] training loss: 0.458\n",
      "Epoch [65] validation loss: 0.444, accuracy: 85.22%\n",
      "LR: [1e-05]\n",
      "Epoch [66] training loss: 0.457\n",
      "Epoch [66] validation loss: 0.443, accuracy: 85.21%\n",
      "LR: [1e-05]\n",
      "Epoch [67] training loss: 0.457\n",
      "Epoch [67] validation loss: 0.444, accuracy: 85.16%\n",
      "LR: [1e-05]\n",
      "Epoch [68] training loss: 0.459\n",
      "Epoch [68] validation loss: 0.445, accuracy: 85.19%\n",
      "LR: [1.0000000000000002e-06]\n",
      "Epoch [69] training loss: 0.456\n",
      "Epoch [69] validation loss: 0.444, accuracy: 85.15%\n",
      "LR: [1.0000000000000002e-06]\n",
      "Epoch [70] training loss: 0.458\n",
      "Epoch [70] validation loss: 0.444, accuracy: 85.15%\n",
      "LR: [1.0000000000000002e-06]\n",
      "Epoch [71] training loss: 0.456\n",
      "Epoch [71] validation loss: 0.444, accuracy: 85.16%\n",
      "LR: [1.0000000000000002e-06]\n",
      "Epoch [72] training loss: 0.456\n",
      "Epoch [72] validation loss: 0.444, accuracy: 85.14%\n",
      "LR: [1.0000000000000002e-07]\n",
      "Epoch [73] training loss: 0.455\n",
      "Epoch [73] validation loss: 0.444, accuracy: 85.15%\n",
      "LR: [1.0000000000000002e-07]\n",
      "Epoch [74] training loss: 0.457\n",
      "Epoch [74] validation loss: 0.444, accuracy: 85.15%\n",
      "LR: [1.0000000000000002e-07]\n",
      "Epoch [75] training loss: 0.455\n",
      "Epoch [75] validation loss: 0.444, accuracy: 85.15%\n",
      "LR: [1.0000000000000002e-07]\n",
      "Epoch [76] training loss: 0.458\n",
      "Epoch [76] validation loss: 0.444, accuracy: 85.16%\n",
      "LR: [1.0000000000000004e-08]\n",
      "Epoch [77] training loss: 0.455\n",
      "Epoch [77] validation loss: 0.444, accuracy: 85.16%\n",
      "LR: [1.0000000000000004e-08]\n",
      "Epoch [78] training loss: 0.458\n",
      "Epoch [78] validation loss: 0.444, accuracy: 85.15%\n",
      "LR: [1.0000000000000004e-08]\n",
      "Epoch [79] training loss: 0.455\n",
      "Epoch [79] validation loss: 0.444, accuracy: 85.16%\n",
      "LR: [1.0000000000000004e-08]\n",
      "Epoch [80] training loss: 0.455\n",
      "Epoch [80] validation loss: 0.444, accuracy: 85.16%\n",
      "LR: [1.0000000000000004e-08]\n",
      "Epoch [81] training loss: 0.459\n",
      "Epoch [81] validation loss: 0.444, accuracy: 85.16%\n",
      "LR: [1.0000000000000004e-08]\n",
      "Epoch [82] training loss: 0.455\n",
      "Epoch [82] validation loss: 0.444, accuracy: 85.16%\n",
      "LR: [1.0000000000000004e-08]\n",
      "Epoch [83] training loss: 0.458\n",
      "Epoch [83] validation loss: 0.444, accuracy: 85.16%\n",
      "LR: [1.0000000000000004e-08]\n",
      "Epoch [84] training loss: 0.457\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [84] validation loss: 0.444, accuracy: 85.16%\n",
      "LR: [1.0000000000000004e-08]\n",
      "Epoch [85] training loss: 0.459\n",
      "Epoch [85] validation loss: 0.444, accuracy: 85.16%\n",
      "LR: [1.0000000000000004e-08]\n",
      "Epoch [86] training loss: 0.456\n",
      "Epoch [86] validation loss: 0.444, accuracy: 85.16%\n",
      "LR: [1.0000000000000004e-08]\n",
      "Epoch [87] training loss: 0.453\n",
      "Epoch [87] validation loss: 0.444, accuracy: 85.16%\n",
      "LR: [1.0000000000000004e-08]\n",
      "Epoch [88] training loss: 0.459\n",
      "Epoch [88] validation loss: 0.444, accuracy: 85.16%\n",
      "LR: [1.0000000000000004e-08]\n",
      "Epoch [89] training loss: 0.457\n",
      "Epoch [89] validation loss: 0.444, accuracy: 85.16%\n",
      "LR: [1.0000000000000004e-08]\n",
      "Epoch [90] training loss: 0.459\n",
      "Epoch [90] validation loss: 0.444, accuracy: 85.16%\n",
      "LR: [1.0000000000000004e-08]\n",
      "Epoch [91] training loss: 0.458\n",
      "Epoch [91] validation loss: 0.444, accuracy: 85.16%\n",
      "LR: [1.0000000000000004e-08]\n",
      "Epoch [92] training loss: 0.454\n",
      "Epoch [92] validation loss: 0.444, accuracy: 85.16%\n",
      "LR: [1.0000000000000004e-08]\n",
      "Epoch [93] training loss: 0.458\n",
      "Epoch [93] validation loss: 0.444, accuracy: 85.16%\n",
      "LR: [1.0000000000000004e-08]\n",
      "Epoch [94] training loss: 0.457\n",
      "Epoch [94] validation loss: 0.444, accuracy: 85.16%\n",
      "LR: [1.0000000000000004e-08]\n",
      "Epoch [95] training loss: 0.456\n",
      "Epoch [95] validation loss: 0.444, accuracy: 85.16%\n",
      "LR: [1.0000000000000004e-08]\n",
      "Epoch [96] training loss: 0.454\n",
      "Epoch [96] validation loss: 0.444, accuracy: 85.16%\n",
      "LR: [1.0000000000000004e-08]\n",
      "Epoch [97] training loss: 0.459\n",
      "Epoch [97] validation loss: 0.444, accuracy: 85.16%\n",
      "LR: [1.0000000000000004e-08]\n",
      "Epoch [98] training loss: 0.460\n",
      "Epoch [98] validation loss: 0.444, accuracy: 85.16%\n",
      "LR: [1.0000000000000004e-08]\n",
      "Epoch [99] training loss: 0.457\n",
      "Epoch [99] validation loss: 0.444, accuracy: 85.16%\n",
      "LR: [1.0000000000000004e-08]\n",
      "Epoch [100] training loss: 0.456\n",
      "Epoch [100] validation loss: 0.444, accuracy: 85.16%\n",
      "LR: [1.0000000000000004e-08]\n"
     ]
    }
   ],
   "source": [
    "train_loss_mem = []\n",
    "val_loss_mem = []\n",
    "val_accuracy_mem = []\n",
    "\n",
    "for epoch in range(100):\n",
    "    base_model_c.train()  # Set model to training mode\n",
    "    running_loss = 0.0\n",
    "    \n",
    "    for i, data in enumerate(train_loader, 0):\n",
    "        inputs, labels = data\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        outputs = base_model_c(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "    \n",
    "    train_loss = running_loss / len(train_loader)\n",
    "    train_loss_mem.append(train_loss)\n",
    "    \n",
    "    print(f'Epoch [{epoch + 1}] training loss: {train_loss:.3f}')\n",
    "    \n",
    "    # Validation phase\n",
    "    base_model_c.eval()  # Set model to evaluation mode\n",
    "    val_running_loss = 0.0\n",
    "    val_correct = 0\n",
    "    val_total = 0\n",
    "    with torch.no_grad():\n",
    "        for data in test_loader:  # Assuming test_loader is used as a validation loader\n",
    "            inputs, labels = data\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            \n",
    "            outputs = base_model_c(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            val_running_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            val_total += labels.size(0)\n",
    "            val_correct += (predicted == labels).sum().item()\n",
    "    \n",
    "    val_loss = val_running_loss / len(test_loader)\n",
    "    val_accuracy = 100 * val_correct / val_total\n",
    "    \n",
    "    print(f'Epoch [{epoch + 1}] validation loss: {val_loss:.3f}, accuracy: {val_accuracy:.2f}%')\n",
    "    val_loss_mem.append(val_loss)\n",
    "    val_accuracy_mem.append(val_accuracy)\n",
    "    # Update the LR scheduler with validation loss\n",
    "    scheduler.step(val_loss)\n",
    "    print(f'LR: {scheduler.get_last_lr()}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89b50ee8",
   "metadata": {},
   "source": [
    "## Define All_CNN_C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bfc66cc0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                   Output Shape              Param #\n",
       "==========================================================================================\n",
       "all_cnn_c                                [1, 10]                   --\n",
       "├─Dropout2d: 1-1                         [1, 3, 32, 32]            --\n",
       "├─Conv2d: 1-2                            [1, 96, 32, 32]           2,688\n",
       "├─ReLU: 1-3                              [1, 96, 32, 32]           --\n",
       "├─Conv2d: 1-4                            [1, 96, 32, 32]           83,040\n",
       "├─ReLU: 1-5                              [1, 96, 32, 32]           --\n",
       "├─Conv2d: 1-6                            [1, 96, 16, 16]           83,040\n",
       "├─ReLU: 1-7                              [1, 96, 16, 16]           --\n",
       "├─Dropout2d: 1-8                         [1, 96, 16, 16]           --\n",
       "├─Conv2d: 1-9                            [1, 192, 16, 16]          166,080\n",
       "├─ReLU: 1-10                             [1, 192, 16, 16]          --\n",
       "├─Conv2d: 1-11                           [1, 192, 16, 16]          331,968\n",
       "├─ReLU: 1-12                             [1, 192, 16, 16]          --\n",
       "├─Conv2d: 1-13                           [1, 192, 8, 8]            331,968\n",
       "├─ReLU: 1-14                             [1, 192, 8, 8]            --\n",
       "├─Dropout2d: 1-15                        [1, 192, 8, 8]            --\n",
       "├─Conv2d: 1-16                           [1, 192, 6, 6]            331,968\n",
       "├─ReLU: 1-17                             [1, 192, 6, 6]            --\n",
       "├─Conv2d: 1-18                           [1, 192, 6, 6]            37,056\n",
       "├─ReLU: 1-19                             [1, 192, 6, 6]            --\n",
       "├─Conv2d: 1-20                           [1, 10, 6, 6]             1,930\n",
       "├─ReLU: 1-21                             [1, 10, 6, 6]             --\n",
       "├─AvgPool2d: 1-22                        [1, 10, 1, 1]             --\n",
       "==========================================================================================\n",
       "Total params: 1,369,738\n",
       "Trainable params: 1,369,738\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (Units.MEGABYTES): 271.14\n",
       "==========================================================================================\n",
       "Input size (MB): 0.01\n",
       "Forward/backward pass size (MB): 2.77\n",
       "Params size (MB): 5.48\n",
       "Estimated Total Size (MB): 8.26\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class all_cnn_c(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.dropout1 = nn.Dropout2d(p=0.2)\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(\n",
    "            in_channels=3, out_channels=96,\n",
    "            kernel_size=(3,3),\n",
    "            stride=1, padding=1,\n",
    "        )\n",
    "        self.relu1 = nn.ReLU()\n",
    "        \n",
    "        self.conv2 = nn.Conv2d(\n",
    "            in_channels=96, out_channels=96,\n",
    "            kernel_size=(3,3),\n",
    "            stride=1, padding=1,\n",
    "        )\n",
    "        self.relu2 = nn.ReLU()\n",
    "        \n",
    "        self.conv3 = nn.Conv2d(\n",
    "            in_channels=96, out_channels=96,\n",
    "            kernel_size=(3,3),\n",
    "            stride=2, padding=1,\n",
    "        ) # Replace MP with this conv\n",
    "        self.relu3 = nn.ReLU()\n",
    "        self.dropout2 = nn.Dropout2d(p=0.5)\n",
    "        \n",
    "        self.conv4 = nn.Conv2d(\n",
    "            in_channels=96, out_channels=192,\n",
    "            kernel_size=(3,3),\n",
    "            stride=1, padding=1,\n",
    "        )\n",
    "        self.relu4 = nn.ReLU()\n",
    "        \n",
    "        self.conv5 = nn.Conv2d(\n",
    "            in_channels=192, out_channels=192,\n",
    "            kernel_size=(3,3),\n",
    "            stride=1, padding=1,\n",
    "        )\n",
    "        self.relu5 = nn.ReLU()\n",
    "        \n",
    "        self.conv6 = nn.Conv2d(\n",
    "            in_channels=192, out_channels=192,\n",
    "            kernel_size=(3,3),\n",
    "            stride=2, padding=1,\n",
    "        ) # Replace MP with this conv\n",
    "        self.relu6 = nn.ReLU()\n",
    "        self.dropout3 = nn.Dropout2d(p=0.5)\n",
    "        \n",
    "        self.conv7 = nn.Conv2d(\n",
    "            in_channels=192, out_channels=192,\n",
    "            kernel_size=(3,3),\n",
    "            stride=1, padding=0,\n",
    "        )\n",
    "        self.relu7 = nn.ReLU()\n",
    "        \n",
    "        self.conv8 = nn.Conv2d(\n",
    "            in_channels=192, out_channels=192,\n",
    "            kernel_size=(1,1),\n",
    "            stride=1, padding=0,\n",
    "        )\n",
    "        self.relu8 = nn.ReLU()\n",
    "        \n",
    "        self.conv9 = nn.Conv2d(\n",
    "            in_channels=192, out_channels=10,\n",
    "            kernel_size=(1,1),\n",
    "            stride=1, padding=0,\n",
    "        )\n",
    "        self.relu9 = nn.ReLU()\n",
    "        \n",
    "        self.avg_pool = nn.AvgPool2d(\n",
    "            kernel_size=(6,6)\n",
    "        )\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.dropout1(x)\n",
    "        #print(f'Shape after dropout1: {x.shape}')\n",
    "        x = self.conv1(x)\n",
    "        x = self.relu1(x)\n",
    "        #print(f'Shape after conv1: {x.shape}')\n",
    "        x = self.conv2(x)\n",
    "        x = self.relu2(x)\n",
    "        #print(f'Shape after conv2: {x.shape}')\n",
    "        x = self.conv3(x)\n",
    "        x = self.relu3(x)\n",
    "        x = self.dropout2(x)\n",
    "        #print(f'Shape after conv3: {x.shape}')\n",
    "        x = self.conv4(x)\n",
    "        x = self.relu4(x)\n",
    "        #print(f'Shape after conv4: {x.shape}')\n",
    "        x = self.conv5(x)\n",
    "        x = self.relu5(x)\n",
    "        #print(f'Shape after conv5: {x.shape}')\n",
    "        x = self.conv6(x)\n",
    "        x = self.relu6(x)\n",
    "        x = self.dropout3(x)\n",
    "        #print(f'Shape after conv1: {x.shape}')\n",
    "        x = self.conv7(x)\n",
    "        x = self.relu7(x)\n",
    "        #print(f'Shape after conv1: {x.shape}')\n",
    "        x = self.conv8(x)\n",
    "        x = self.relu8(x)\n",
    "        #print(f'Shape after conv1: {x.shape}')\n",
    "        x = self.conv9(x)\n",
    "        x = self.relu9(x)\n",
    "        #print(f'Shape after conv1: {x.shape}')\n",
    "        x = self.avg_pool(x)\n",
    "        #print(f'Shape after global pool layer: {x.shape}')\n",
    "        x = torch.flatten(x, start_dim=1)\n",
    "        #print(f'out shape: {x.shape}')\n",
    "        return x\n",
    "\n",
    "def glorot_uniform_init(net):\n",
    "    if isinstance(net, nn.Conv2d):\n",
    "        nn.init.xavier_normal_(net.weight)\n",
    "        if net.bias is not None:\n",
    "            nn.init.constant_(net.bias, 0)\n",
    "\n",
    "all_cnn_c_model = all_cnn_c()\n",
    "all_cnn_c_model.apply(glorot_uniform_init)\n",
    "all_cnn_c_model = all_cnn_c_model.to(device)\n",
    "summary(all_cnn_c_model, input_size=(1, 3, 32, 32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7782e8b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_in = torch.randn(1, 3, 32, 32)\n",
    "dummy_in = dummy_in.to(device)\n",
    "out = all_cnn_c_model(dummy_in)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a299669b",
   "metadata": {},
   "source": [
    "### ALL_CNN_C on Cifar10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "59ca81e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "from torchvision.datasets import CIFAR10\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    #transforms.Resize(224), # Not needed for All CNN paper\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.4914, 0.48227 ,0.4465], std=[0.2470, 0.2435, 0.2616]), \n",
    "])\n",
    "\n",
    "# Load datasets\n",
    "train_dataset = CIFAR10(root='./CIFAR', train=True, download=True, transform=transform)\n",
    "test_dataset = CIFAR10(root='./CIFAR', train=False, download=True, transform=transform)\n",
    "\n",
    "# Create data loaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True, num_workers=4)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ad463daf",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(all_cnn_c_model.parameters(), lr=0.01, weight_decay=0.001, momentum=0.9)\n",
    "\n",
    "# Define learning rate scheduler\n",
    "scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "faf9f553",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1] training loss: 2.151\n",
      "Epoch [1] validation loss: 1.883, accuracy: 27.41%\n",
      "LR: [0.01]\n",
      "Epoch [2] training loss: 1.870\n",
      "Epoch [2] validation loss: 1.708, accuracy: 37.24%\n",
      "LR: [0.01]\n",
      "Epoch [3] training loss: 1.725\n",
      "Epoch [3] validation loss: 1.777, accuracy: 36.21%\n",
      "LR: [0.01]\n",
      "Epoch [4] training loss: 1.633\n",
      "Epoch [4] validation loss: 1.512, accuracy: 44.47%\n",
      "LR: [0.01]\n",
      "Epoch [5] training loss: 1.550\n",
      "Epoch [5] validation loss: 1.428, accuracy: 46.92%\n",
      "LR: [0.01]\n",
      "Epoch [6] training loss: 1.466\n",
      "Epoch [6] validation loss: 1.387, accuracy: 49.12%\n",
      "LR: [0.01]\n",
      "Epoch [7] training loss: 1.401\n",
      "Epoch [7] validation loss: 1.268, accuracy: 54.74%\n",
      "LR: [0.01]\n",
      "Epoch [8] training loss: 1.340\n",
      "Epoch [8] validation loss: 1.269, accuracy: 55.94%\n",
      "LR: [0.01]\n",
      "Epoch [9] training loss: 1.270\n",
      "Epoch [9] validation loss: 1.123, accuracy: 59.79%\n",
      "LR: [0.01]\n",
      "Epoch [10] training loss: 1.225\n",
      "Epoch [10] validation loss: 1.129, accuracy: 59.80%\n",
      "LR: [0.01]\n",
      "Epoch [11] training loss: 1.178\n",
      "Epoch [11] validation loss: 1.036, accuracy: 64.12%\n",
      "LR: [0.01]\n",
      "Epoch [12] training loss: 1.128\n",
      "Epoch [12] validation loss: 0.996, accuracy: 65.37%\n",
      "LR: [0.01]\n",
      "Epoch [13] training loss: 1.087\n",
      "Epoch [13] validation loss: 0.953, accuracy: 66.78%\n",
      "LR: [0.01]\n",
      "Epoch [14] training loss: 1.045\n",
      "Epoch [14] validation loss: 0.980, accuracy: 66.21%\n",
      "LR: [0.01]\n",
      "Epoch [15] training loss: 1.016\n",
      "Epoch [15] validation loss: 0.955, accuracy: 67.37%\n",
      "LR: [0.01]\n",
      "Epoch [16] training loss: 0.981\n",
      "Epoch [16] validation loss: 0.860, accuracy: 70.29%\n",
      "LR: [0.01]\n",
      "Epoch [17] training loss: 0.953\n",
      "Epoch [17] validation loss: 0.872, accuracy: 69.96%\n",
      "LR: [0.01]\n",
      "Epoch [18] training loss: 0.934\n",
      "Epoch [18] validation loss: 0.776, accuracy: 73.05%\n",
      "LR: [0.01]\n",
      "Epoch [19] training loss: 0.909\n",
      "Epoch [19] validation loss: 0.762, accuracy: 73.53%\n",
      "LR: [0.01]\n",
      "Epoch [20] training loss: 0.897\n",
      "Epoch [20] validation loss: 0.814, accuracy: 71.36%\n",
      "LR: [0.01]\n",
      "Epoch [21] training loss: 0.870\n",
      "Epoch [21] validation loss: 0.747, accuracy: 73.76%\n",
      "LR: [0.01]\n",
      "Epoch [22] training loss: 0.849\n",
      "Epoch [22] validation loss: 0.732, accuracy: 75.11%\n",
      "LR: [0.01]\n",
      "Epoch [23] training loss: 0.828\n",
      "Epoch [23] validation loss: 0.793, accuracy: 72.75%\n",
      "LR: [0.01]\n",
      "Epoch [24] training loss: 0.811\n",
      "Epoch [24] validation loss: 0.714, accuracy: 75.37%\n",
      "LR: [0.01]\n",
      "Epoch [25] training loss: 0.799\n",
      "Epoch [25] validation loss: 0.670, accuracy: 76.93%\n",
      "LR: [0.01]\n",
      "Epoch [26] training loss: 0.788\n",
      "Epoch [26] validation loss: 0.734, accuracy: 74.14%\n",
      "LR: [0.01]\n",
      "Epoch [27] training loss: 0.771\n",
      "Epoch [27] validation loss: 0.651, accuracy: 78.32%\n",
      "LR: [0.01]\n",
      "Epoch [28] training loss: 0.756\n",
      "Epoch [28] validation loss: 0.681, accuracy: 76.20%\n",
      "LR: [0.01]\n",
      "Epoch [29] training loss: 0.743\n",
      "Epoch [29] validation loss: 0.653, accuracy: 77.81%\n",
      "LR: [0.01]\n",
      "Epoch [30] training loss: 0.729\n",
      "Epoch [30] validation loss: 0.623, accuracy: 78.45%\n",
      "LR: [0.01]\n",
      "Epoch [31] training loss: 0.725\n",
      "Epoch [31] validation loss: 0.749, accuracy: 73.43%\n",
      "LR: [0.01]\n",
      "Epoch [32] training loss: 0.713\n",
      "Epoch [32] validation loss: 0.589, accuracy: 79.85%\n",
      "LR: [0.01]\n",
      "Epoch [33] training loss: 0.704\n",
      "Epoch [33] validation loss: 0.607, accuracy: 79.30%\n",
      "LR: [0.01]\n",
      "Epoch [34] training loss: 0.697\n",
      "Epoch [34] validation loss: 0.569, accuracy: 80.99%\n",
      "LR: [0.01]\n",
      "Epoch [35] training loss: 0.686\n",
      "Epoch [35] validation loss: 0.580, accuracy: 80.09%\n",
      "LR: [0.01]\n",
      "Epoch [36] training loss: 0.677\n",
      "Epoch [36] validation loss: 0.596, accuracy: 79.55%\n",
      "LR: [0.01]\n",
      "Epoch [37] training loss: 0.668\n",
      "Epoch [37] validation loss: 0.571, accuracy: 80.61%\n",
      "LR: [0.01]\n",
      "Epoch [38] training loss: 0.663\n",
      "Epoch [38] validation loss: 0.585, accuracy: 79.82%\n",
      "LR: [0.001]\n",
      "Epoch [39] training loss: 0.556\n",
      "Epoch [39] validation loss: 0.510, accuracy: 82.72%\n",
      "LR: [0.001]\n",
      "Epoch [40] training loss: 0.525\n",
      "Epoch [40] validation loss: 0.500, accuracy: 83.27%\n",
      "LR: [0.001]\n",
      "Epoch [41] training loss: 0.518\n",
      "Epoch [41] validation loss: 0.505, accuracy: 82.78%\n",
      "LR: [0.001]\n",
      "Epoch [42] training loss: 0.512\n",
      "Epoch [42] validation loss: 0.498, accuracy: 82.97%\n",
      "LR: [0.001]\n",
      "Epoch [43] training loss: 0.506\n",
      "Epoch [43] validation loss: 0.486, accuracy: 83.48%\n",
      "LR: [0.001]\n",
      "Epoch [44] training loss: 0.499\n",
      "Epoch [44] validation loss: 0.490, accuracy: 83.17%\n",
      "LR: [0.001]\n",
      "Epoch [45] training loss: 0.500\n",
      "Epoch [45] validation loss: 0.483, accuracy: 83.55%\n",
      "LR: [0.001]\n",
      "Epoch [46] training loss: 0.495\n",
      "Epoch [46] validation loss: 0.473, accuracy: 83.91%\n",
      "LR: [0.001]\n",
      "Epoch [47] training loss: 0.494\n",
      "Epoch [47] validation loss: 0.471, accuracy: 83.73%\n",
      "LR: [0.001]\n",
      "Epoch [48] training loss: 0.490\n",
      "Epoch [48] validation loss: 0.472, accuracy: 84.22%\n",
      "LR: [0.001]\n",
      "Epoch [49] training loss: 0.488\n",
      "Epoch [49] validation loss: 0.471, accuracy: 84.06%\n",
      "LR: [0.001]\n",
      "Epoch [50] training loss: 0.486\n",
      "Epoch [50] validation loss: 0.472, accuracy: 83.98%\n",
      "LR: [0.001]\n",
      "Epoch [51] training loss: 0.483\n",
      "Epoch [51] validation loss: 0.469, accuracy: 83.95%\n",
      "LR: [0.001]\n",
      "Epoch [52] training loss: 0.480\n",
      "Epoch [52] validation loss: 0.476, accuracy: 83.65%\n",
      "LR: [0.001]\n",
      "Epoch [53] training loss: 0.480\n",
      "Epoch [53] validation loss: 0.488, accuracy: 83.25%\n",
      "LR: [0.001]\n",
      "Epoch [54] training loss: 0.481\n",
      "Epoch [54] validation loss: 0.465, accuracy: 84.27%\n",
      "LR: [0.001]\n",
      "Epoch [55] training loss: 0.477\n",
      "Epoch [55] validation loss: 0.462, accuracy: 84.12%\n",
      "LR: [0.001]\n",
      "Epoch [56] training loss: 0.475\n",
      "Epoch [56] validation loss: 0.462, accuracy: 84.25%\n",
      "LR: [0.001]\n",
      "Epoch [57] training loss: 0.477\n",
      "Epoch [57] validation loss: 0.458, accuracy: 84.45%\n",
      "LR: [0.001]\n",
      "Epoch [58] training loss: 0.473\n",
      "Epoch [58] validation loss: 0.467, accuracy: 84.08%\n",
      "LR: [0.001]\n",
      "Epoch [59] training loss: 0.473\n",
      "Epoch [59] validation loss: 0.462, accuracy: 84.26%\n",
      "LR: [0.001]\n",
      "Epoch [60] training loss: 0.468\n",
      "Epoch [60] validation loss: 0.458, accuracy: 84.27%\n",
      "LR: [0.001]\n",
      "Epoch [61] training loss: 0.469\n",
      "Epoch [61] validation loss: 0.470, accuracy: 83.86%\n",
      "LR: [0.001]\n",
      "Epoch [62] training loss: 0.470\n",
      "Epoch [62] validation loss: 0.464, accuracy: 84.09%\n",
      "LR: [0.001]\n",
      "Epoch [63] training loss: 0.463\n",
      "Epoch [63] validation loss: 0.453, accuracy: 84.47%\n",
      "LR: [0.001]\n",
      "Epoch [64] training loss: 0.463\n",
      "Epoch [64] validation loss: 0.473, accuracy: 83.94%\n",
      "LR: [0.001]\n",
      "Epoch [65] training loss: 0.466\n",
      "Epoch [65] validation loss: 0.472, accuracy: 84.02%\n",
      "LR: [0.001]\n",
      "Epoch [66] training loss: 0.463\n",
      "Epoch [66] validation loss: 0.447, accuracy: 84.61%\n",
      "LR: [0.001]\n",
      "Epoch [67] training loss: 0.460\n",
      "Epoch [67] validation loss: 0.458, accuracy: 84.34%\n",
      "LR: [0.001]\n",
      "Epoch [68] training loss: 0.460\n",
      "Epoch [68] validation loss: 0.458, accuracy: 84.40%\n",
      "LR: [0.001]\n",
      "Epoch [69] training loss: 0.459\n",
      "Epoch [69] validation loss: 0.455, accuracy: 84.66%\n",
      "LR: [0.001]\n",
      "Epoch [70] training loss: 0.460\n",
      "Epoch [70] validation loss: 0.465, accuracy: 84.17%\n",
      "LR: [0.0001]\n",
      "Epoch [71] training loss: 0.440\n",
      "Epoch [71] validation loss: 0.448, accuracy: 84.67%\n",
      "LR: [0.0001]\n",
      "Epoch [72] training loss: 0.436\n",
      "Epoch [72] validation loss: 0.444, accuracy: 84.84%\n",
      "LR: [0.0001]\n",
      "Epoch [73] training loss: 0.434\n",
      "Epoch [73] validation loss: 0.442, accuracy: 84.78%\n",
      "LR: [0.0001]\n",
      "Epoch [74] training loss: 0.431\n",
      "Epoch [74] validation loss: 0.444, accuracy: 84.85%\n",
      "LR: [0.0001]\n",
      "Epoch [75] training loss: 0.433\n",
      "Epoch [75] validation loss: 0.443, accuracy: 84.82%\n",
      "LR: [0.0001]\n",
      "Epoch [76] training loss: 0.431\n",
      "Epoch [76] validation loss: 0.448, accuracy: 84.74%\n",
      "LR: [0.0001]\n",
      "Epoch [77] training loss: 0.431\n",
      "Epoch [77] validation loss: 0.443, accuracy: 84.77%\n",
      "LR: [1e-05]\n",
      "Epoch [78] training loss: 0.427\n",
      "Epoch [78] validation loss: 0.442, accuracy: 84.82%\n",
      "LR: [1e-05]\n",
      "Epoch [79] training loss: 0.429\n",
      "Epoch [79] validation loss: 0.442, accuracy: 84.90%\n",
      "LR: [1e-05]\n",
      "Epoch [80] training loss: 0.426\n",
      "Epoch [80] validation loss: 0.442, accuracy: 84.86%\n",
      "LR: [1e-05]\n",
      "Epoch [81] training loss: 0.423\n",
      "Epoch [81] validation loss: 0.442, accuracy: 84.85%\n",
      "LR: [1e-05]\n",
      "Epoch [82] training loss: 0.427\n",
      "Epoch [82] validation loss: 0.442, accuracy: 84.83%\n",
      "LR: [1e-05]\n",
      "Epoch [83] training loss: 0.429\n",
      "Epoch [83] validation loss: 0.442, accuracy: 84.81%\n",
      "LR: [1e-05]\n",
      "Epoch [84] training loss: 0.425\n",
      "Epoch [84] validation loss: 0.443, accuracy: 84.87%\n",
      "LR: [1.0000000000000002e-06]\n",
      "Epoch [85] training loss: 0.426\n",
      "Epoch [85] validation loss: 0.443, accuracy: 84.86%\n",
      "LR: [1.0000000000000002e-06]\n",
      "Epoch [86] training loss: 0.422\n",
      "Epoch [86] validation loss: 0.442, accuracy: 84.87%\n",
      "LR: [1.0000000000000002e-06]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [87] training loss: 0.427\n",
      "Epoch [87] validation loss: 0.442, accuracy: 84.88%\n",
      "LR: [1.0000000000000002e-06]\n",
      "Epoch [88] training loss: 0.426\n",
      "Epoch [88] validation loss: 0.442, accuracy: 84.85%\n",
      "LR: [1.0000000000000002e-07]\n",
      "Epoch [89] training loss: 0.425\n",
      "Epoch [89] validation loss: 0.442, accuracy: 84.85%\n",
      "LR: [1.0000000000000002e-07]\n",
      "Epoch [90] training loss: 0.422\n",
      "Epoch [90] validation loss: 0.442, accuracy: 84.85%\n",
      "LR: [1.0000000000000002e-07]\n",
      "Epoch [91] training loss: 0.428\n",
      "Epoch [91] validation loss: 0.442, accuracy: 84.85%\n",
      "LR: [1.0000000000000002e-07]\n",
      "Epoch [92] training loss: 0.426\n",
      "Epoch [92] validation loss: 0.442, accuracy: 84.85%\n",
      "LR: [1.0000000000000004e-08]\n",
      "Epoch [93] training loss: 0.426\n",
      "Epoch [93] validation loss: 0.442, accuracy: 84.85%\n",
      "LR: [1.0000000000000004e-08]\n",
      "Epoch [94] training loss: 0.423\n",
      "Epoch [94] validation loss: 0.442, accuracy: 84.85%\n",
      "LR: [1.0000000000000004e-08]\n",
      "Epoch [95] training loss: 0.429\n",
      "Epoch [95] validation loss: 0.442, accuracy: 84.85%\n",
      "LR: [1.0000000000000004e-08]\n",
      "Epoch [96] training loss: 0.427\n",
      "Epoch [96] validation loss: 0.442, accuracy: 84.85%\n",
      "LR: [1.0000000000000004e-08]\n",
      "Epoch [97] training loss: 0.426\n",
      "Epoch [97] validation loss: 0.442, accuracy: 84.85%\n",
      "LR: [1.0000000000000004e-08]\n",
      "Epoch [98] training loss: 0.428\n",
      "Epoch [98] validation loss: 0.442, accuracy: 84.85%\n",
      "LR: [1.0000000000000004e-08]\n",
      "Epoch [99] training loss: 0.426\n",
      "Epoch [99] validation loss: 0.442, accuracy: 84.85%\n",
      "LR: [1.0000000000000004e-08]\n",
      "Epoch [100] training loss: 0.425\n",
      "Epoch [100] validation loss: 0.442, accuracy: 84.85%\n",
      "LR: [1.0000000000000004e-08]\n"
     ]
    }
   ],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(all_cnn_c_model.parameters(), lr=0.01, weight_decay=0.001, momentum=0.9)\n",
    "\n",
    "# Define learning rate scheduler\n",
    "scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=3)\n",
    "\n",
    "train_loss_mem = []\n",
    "val_loss_mem = []\n",
    "val_accuracy_mem = []\n",
    "\n",
    "for epoch in range(100):\n",
    "    all_cnn_c_model.train()  # Set model to training mode\n",
    "    running_loss = 0.0\n",
    "    \n",
    "    for i, data in enumerate(train_loader, 0):\n",
    "        inputs, labels = data\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        outputs = all_cnn_c_model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "    \n",
    "    train_loss = running_loss / len(train_loader)\n",
    "    train_loss_mem.append(train_loss)\n",
    "    \n",
    "    print(f'Epoch [{epoch + 1}] training loss: {train_loss:.3f}')\n",
    "    \n",
    "    # Validation phase\n",
    "    all_cnn_c_model.eval()  # Set model to evaluation mode\n",
    "    val_running_loss = 0.0\n",
    "    val_correct = 0\n",
    "    val_total = 0\n",
    "    with torch.no_grad():\n",
    "        for data in test_loader:  # Assuming test_loader is used as a validation loader\n",
    "            inputs, labels = data\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            \n",
    "            outputs = all_cnn_c_model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            val_running_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            val_total += labels.size(0)\n",
    "            val_correct += (predicted == labels).sum().item()\n",
    "    \n",
    "    val_loss = val_running_loss / len(test_loader)\n",
    "    val_accuracy = 100 * val_correct / val_total\n",
    "    \n",
    "    print(f'Epoch [{epoch + 1}] validation loss: {val_loss:.3f}, accuracy: {val_accuracy:.2f}%')\n",
    "    val_loss_mem.append(val_loss)\n",
    "    val_accuracy_mem.append(val_accuracy)\n",
    "    # Update the LR scheduler with validation loss\n",
    "    scheduler.step(val_loss)\n",
    "    print(f'LR: {scheduler.get_last_lr()}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3df6c994",
   "metadata": {},
   "source": [
    "## Deconv from the paper (the fun stuff!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef38ee2c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpu_use",
   "language": "python",
   "name": "gpu_use"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
